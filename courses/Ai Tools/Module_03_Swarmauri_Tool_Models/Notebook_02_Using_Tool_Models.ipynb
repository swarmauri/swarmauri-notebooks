{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Working with Swarmauri Tool Models**  \n",
    "\n",
    "In this notebook, we’ll explore how to effectively use the ToolModels provided by the Swarmauri library. ToolModels are powerful components designed to integrate various tools seamlessly, enabling developers to achieve complex tasks with minimal effort.  \n",
    "\n",
    "We’ll walk through the key features and capabilities of ToolModels, demonstrating how they can simplify workflows and enhance productivity. By the end of this notebook, you’ll have a clear understanding of how to:  \n",
    "- Utilize ToolModels for common use cases.  \n",
    "- Integrate them with other Swarmauri components.  \n",
    "- Leverage their features to streamline your development process.  \n",
    "\n",
    "Let’s dive in and unlock the potential of Swarmauri ToolModels!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup and Configuration**\n",
    "\n",
    "## Import the OpenAIToolModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.llms.concrete.OpenAIToolModel import OpenAIToolModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load your API KEY from your environment variables**\n",
    "- Make sure you have python-dotenv installed if not, run `pip install python-dotenv` so you can install it.\n",
    "- Get your API KEY [HERE](https://platform.openai.com/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initialize OpenAIToolModel**\n",
    "- Note: You can as well input your api key directly, but it's better to load from env file\n",
    "- Also, the `name` arguments is an optional argument that allows you to input a model from the list of allowed_models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_tool_model = OpenAIToolModel(api_key=OPENAI_API_KEY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have initialized the `AnthropicToolModel`, you can use it to interact with Groq's LLM-powered tools. The `AnthropicToolModel` acts as the core engine that processes queries and generates results. Before you can perform a tool call, you need to set up a toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set up a Toolkit**\n",
    "A `Toolkit` is a collection of tools that the model can use to answer queries. Think of it as an organized toolbox containing specialized tools for solving different tasks. In this example, we use the `AdditionTool`, which allows the agent to perform arithmetic operations like addition.  \n",
    "\n",
    "To set up a toolkit:  \n",
    "1. Instantiate the `Toolkit` class.  \n",
    "2. Add specific tools (like `AdditionTool`) to the toolkit using its `add_tool()` method.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.tools.concrete.AdditionTool import AdditionTool #This is the AdittionTool, we already covered tools in the first module\n",
    "from swarmauri.toolkits.concrete.Toolkit import Toolkit  #This is the actual Toolkit class\n",
    "\n",
    "toolkit = Toolkit()\n",
    "addition_tool = AdditionTool()\n",
    "toolkit.add_tool(addition_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create a Conversation**\n",
    "A `Conversation` object stores the sequence of messages exchanged between the user and the model. This allows the model to maintain context and continuity.  \n",
    "\n",
    "To create a conversation:  \n",
    "1. Instantiate the `Conversation` class.  \n",
    "2. Add a message from the user using the `HumanMessage` class.  \n",
    "\n",
    "In this example, we ask, \"What is the sum of 512 and 671?\" and add it as a `HumanMessage` to the conversation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.conversations.concrete.Conversation import Conversation \n",
    "from swarmauri.messages.concrete import HumanMessage # This the class that allows you add your question/prompt\n",
    "\n",
    "conversation = Conversation()\n",
    "input_query = \"What is the sum of 812 and 700?\"\n",
    "conversation.add_message(HumanMessage(content=input_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = openai_tool_model.predict(conversation=conversation, toolkit=toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of 812 and 700 is 1512.\n"
     ]
    }
   ],
   "source": [
    "print(answer.get_last().content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the tool model was able to use the `AdditionTool` to find the sum of 812 and 700. It specifically pointed to the Addition Tool before answering the question\n",
    "Now, let's make it even more interesting, let's use another Tool from the the Tools provided at `swarmauri/tools/concrete`.\n",
    "\n",
    "---\n",
    "\n",
    "### Using JSONRequestTool\n",
    "\n",
    "Below we are going to use the `JSONRequestTool`. A tool that leverages the `requests` library to perform HTTP operations. We are going to use it together with `OpenAIToolModel` so that `OpenAIToolModel` will be able to make HTTP requests for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the JSONRequestTool Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.tools.concrete.JSONRequestsTool import JSONRequestsTool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a toolkit instance and add the JSONRequestsTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = Toolkit()\n",
    "json_requests_tool = JSONRequestsTool()\n",
    "toolkit.add_tool(json_requests_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a conversation with a real API endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = Conversation()\n",
    "input_query = \"Can you make a GET request to https://jsonplaceholder.typicode.com/posts/1 and give me just the JSON data gotten from the URL?\"\n",
    "conversation.add_message(HumanMessage(content=input_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the response using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the JSON data retrieved from the URL https://jsonplaceholder.typicode.com/posts/1:\n",
      "\n",
      "{\n",
      "  \"userId\": 1,\n",
      "  \"id\": 1,\n",
      "  \"title\": \"sunt aut facere repellat provident occaecati excepturi optio reprehenderit\",\n",
      "  \"body\": \"quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "answer = openai_tool_model.predict(conversation=conversation, toolkit=toolkit)\n",
    "print(answer.get_last().content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model was able to make the GET request to the URL and fetch the JSON data from there.\n",
    "This is the beauty of integrating a tool into a ToolModel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aside using the `predict method`, you can also use the following methods\n",
    "- apredict: For asynchronous prediction\n",
    "- stream: To stream the answers as they are being generated\n",
    "- astream: To asynchronously stream the answers as they are being generated.\n",
    "- batch: To process a batch of conversation in a synchronous manner\n",
    "- abatch: To process a batch of conversation asynchronoulsy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use the `apredict` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data retrieved from the URL https://jsonplaceholder.typicode.com/posts/1 is as follows:\n",
      "- userId: 1\n",
      "- id: 1\n",
      "- title: sunt aut facere repellat provident occaecati excepturi optio reprehenderit\n",
      "- body: quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\n"
     ]
    }
   ],
   "source": [
    "result = await openai_tool_model.apredict(\n",
    "    conversation=conversation, toolkit=toolkit\n",
    ")\n",
    "print(result.get_last().content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use the `stream` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The\n",
      " data\n",
      " retrieved\n",
      " from\n",
      " the\n",
      " URL\n",
      " https\n",
      "://\n",
      "json\n",
      "placeholder\n",
      ".typ\n",
      "icode\n",
      ".com\n",
      "/posts\n",
      "/\n",
      "1\n",
      " is\n",
      " as\n",
      " follows\n",
      ":\n",
      "\n",
      "-\n",
      " userId\n",
      ":\n",
      " \n",
      "1\n",
      "\n",
      "\n",
      "-\n",
      " id\n",
      ":\n",
      " \n",
      "1\n",
      "\n",
      "\n",
      "-\n",
      " title\n",
      ":\n",
      " sunt\n",
      " aut\n",
      " fac\n",
      "ere\n",
      " repell\n",
      "at\n",
      " provid\n",
      "ent\n",
      " occ\n",
      "aec\n",
      "ati\n",
      " except\n",
      "uri\n",
      " opt\n",
      "io\n",
      " repreh\n",
      "enderit\n",
      "\n",
      "\n",
      "-\n",
      " body\n",
      ":\n",
      " qu\n",
      "ia\n",
      " et\n",
      " sus\n",
      "cip\n",
      "it\n",
      "\\n\n",
      "s\n",
      "usc\n",
      "ip\n",
      "it\n",
      " rec\n",
      "us\n",
      "and\n",
      "ae\n",
      " consequ\n",
      "unt\n",
      "ur\n",
      " exp\n",
      "edit\n",
      "a\n",
      " et\n",
      " cum\n",
      "\\n\n",
      "re\n",
      "preh\n",
      "enderit\n",
      " molest\n",
      "iae\n",
      " ut\n",
      " ut\n",
      " qu\n",
      "as\n",
      " tot\n",
      "am\n",
      "\\n\n",
      "no\n",
      "str\n",
      "um\n",
      " rer\n",
      "um\n",
      " est\n",
      " aut\n",
      "em\n",
      " sunt\n",
      " rem\n",
      " even\n",
      "iet\n",
      " architect\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "results = openai_tool_model.stream(conversation=conversation, toolkit=toolkit)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use the `astream` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The\n",
      " data\n",
      " retrieved\n",
      " from\n",
      " the\n",
      " URL\n",
      " https\n",
      "://\n",
      "json\n",
      "placeholder\n",
      ".typ\n",
      "icode\n",
      ".com\n",
      "/posts\n",
      "/\n",
      "1\n",
      " is\n",
      " as\n",
      " follows\n",
      ":\n",
      "\n",
      "-\n",
      " userId\n",
      ":\n",
      " \n",
      "1\n",
      "\n",
      "\n",
      "-\n",
      " id\n",
      ":\n",
      " \n",
      "1\n",
      "\n",
      "\n",
      "-\n",
      " title\n",
      ":\n",
      " sunt\n",
      " aut\n",
      " fac\n",
      "ere\n",
      " repell\n",
      "at\n",
      " provid\n",
      "ent\n",
      " occ\n",
      "aec\n",
      "ati\n",
      " except\n",
      "uri\n",
      " opt\n",
      "io\n",
      " repreh\n",
      "enderit\n",
      "\n",
      "\n",
      "-\n",
      " body\n",
      ":\n",
      " qu\n",
      "ia\n",
      " et\n",
      " sus\n",
      "cip\n",
      "it\n",
      "\\n\n",
      "s\n",
      "usc\n",
      "ip\n",
      "it\n",
      " rec\n",
      "us\n",
      "and\n",
      "ae\n",
      " consequ\n",
      "unt\n",
      "ur\n",
      " exp\n",
      "edit\n",
      "a\n",
      " et\n",
      " cum\n",
      "\\n\n",
      "re\n",
      "preh\n",
      "enderit\n",
      " molest\n",
      "iae\n",
      " ut\n",
      " ut\n",
      " qu\n",
      "as\n",
      " tot\n",
      "am\n",
      "\\n\n",
      "no\n",
      "str\n",
      "um\n",
      " rer\n",
      "um\n",
      " est\n",
      " aut\n",
      "em\n",
      " sunt\n",
      " rem\n",
      " even\n",
      "iet\n",
      " architect\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "async for result in openai_tool_model.astream(\n",
    "    conversation=conversation, toolkit=toolkit\n",
    "):\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use the `batch` method \n",
    "You need to create a list containing the prompts you want to create and then add the prompts as messages to the conversation object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for Prompt 1:\n",
      "Here is the JSON data obtained from the URL https://jsonplaceholder.typicode.com/posts/1:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"userId\": 1,\n",
      "  \"id\": 1,\n",
      "  \"title\": \"sunt aut facere repellat provident occaecati excepturi optio reprehenderit\",\n",
      "  \"body\": \"quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\"\n",
      "}\n",
      "```\n",
      "\n",
      "Response for Prompt 2:\n",
      "Here is the JSON data obtained from the URL https://jsonplaceholder.typicode.com/posts/2:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"userId\": 1,\n",
      "  \"id\": 2,\n",
      "  \"title\": \"qui est esse\",\n",
      "  \"body\": \"est rerum tempore vitae\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\nqui aperiam non debitis possimus qui neque nisi nulla\"\n",
      "}\n",
      "```\n",
      "\n",
      "Response for Prompt 3:\n",
      "Here is the JSON data retrieved from the URL https://jsonplaceholder.typicode.com/posts/3:\n",
      "\n",
      "{\n",
      "  \"userId\": 1,\n",
      "  \"id\": 3,\n",
      "  \"title\": \"ea molestias quasi exercitationem repellat qui ipsa sit aut\",\n",
      "  \"body\": \"et iusto sed quo iure\\nvoluptatem occaecati omnis eligendi aut ad\\nvoluptatem doloribus vel accusantium quis pariatur\\nmolestiae porro eius odio et labore et velit aut\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversations = []\n",
    "prompts = [\n",
    "    \"Can you make a GET request to https://jsonplaceholder.typicode.com/posts/1 and give me just the JSON data gotten from the URL?\",\n",
    "    \"Can you make a GET request to https://jsonplaceholder.typicode.com/posts/2 and give me just the JSON data gotten from the URL?\",\n",
    "    \"Can you make a GET request to https://jsonplaceholder.typicode.com/posts/3 and give me just the JSON data gotten from the URL?\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    conv = Conversation()\n",
    "    conv.add_message(HumanMessage(content=prompt))\n",
    "    conversations.append(conv)\n",
    "\n",
    "# Call the batch method\n",
    "results = openai_tool_model.batch(conversations=conversations, toolkit=toolkit)\n",
    "\n",
    "# Print the results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Response for Prompt {i + 1}:\\n{result.get_last().content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name=None id='75325517-33f4-4b6a-928b-93cd40a672df' members=[] owner=None host=None resource='Conversation' version='0.1.0' type='Conversation'\n",
      "name=None id='4d9bfc5d-81b4-4c6e-9775-3da8aeb7475c' members=[] owner=None host=None resource='Conversation' version='0.1.0' type='Conversation'\n",
      "name=None id='703f089c-5490-4908-8110-c69b57d817a4' members=[] owner=None host=None resource='Conversation' version='0.1.0' type='Conversation'\n"
     ]
    }
   ],
   "source": [
    "results = anthropic_tool_model.batch(conversations=conversations, toolkit=toolkit)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use the `abatch` method \n",
    "You need to create a list containing the prompts you want to create and then add the prompts as messages to the conversation object.\n",
    "But now, instead of using the `batch` method, you will use the `abatch` method and `await` the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for Prompt 1:\n",
      "The JSON data obtained from the URL https://jsonplaceholder.typicode.com/posts/1 is as follows:\n",
      "\n",
      "{\n",
      "  \"userId\": 1,\n",
      "  \"id\": 1,\n",
      "  \"title\": \"sunt aut facere repellat provident occaecati excepturi optio reprehenderit\",\n",
      "  \"body\": \"quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\"\n",
      "}\n",
      "\n",
      "Response for Prompt 2:\n",
      "The JSON data obtained from the URL https://jsonplaceholder.typicode.com/posts/2 is as follows:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"userId\": 1,\n",
      "  \"id\": 2,\n",
      "  \"title\": \"qui est esse\",\n",
      "  \"body\": \"est rerum tempore vitae\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\nqui aperiam non debitis possimus qui neque nisi nulla\"\n",
      "}\n",
      "```\n",
      "\n",
      "Response for Prompt 3:\n",
      "The JSON data retrieved from the URL https://jsonplaceholder.typicode.com/posts/3 is as follows:\n",
      "\n",
      "{\n",
      "  \"userId\": 1,\n",
      "  \"id\": 3,\n",
      "  \"title\": \"ea molestias quasi exercitationem repellat qui ipsa sit aut\",\n",
      "  \"body\": \"et iusto sed quo iure\\nvoluptatem occaecati omnis eligendi aut ad\\nvoluptatem doloribus vel accusantium quis pariatur\\nmolestiae porro eius odio et labore et velit aut\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = await openai_tool_model.abatch(conversations=conversations, toolkit=toolkit)\n",
    "\n",
    "# Print the results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Response for Prompt {i + 1}:\\n{result.get_last().content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "In this notebook, we have covered how to use `ToolModels` in swarmauri, uisng OpenAIToolModel as an example.\n",
    "With this, you can efficiently use any toolmodel of your choice effectively. Make sure to check out the Tools in `swarmauri/tools/concrete` and the ones in `swarmauri_community/tools/concrete`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTEBOOK METADATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Victory Nnaji\n",
      "GitHub Username: 3rd-Son\n",
      "Notebook File: Notebook_02_Using_Tool_Models.ipynb\n",
      "Last Modified: 2025-01-03 12:37:57.388445\n",
      "Platform: Darwin 24.1.0\n",
      "Python Version: 3.11.11 (main, Dec 11 2024, 10:25:04) [Clang 14.0.6 ]\n",
      "Swarmauri Version: 0.5.2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.utils import print_notebook_metadata\n",
    "\n",
    "metadata = print_notebook_metadata.print_notebook_metadata(\"Victory Nnaji\", \"3rd-Son\")\n",
    "print(metadata) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri-notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
