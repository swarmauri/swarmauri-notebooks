{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Notebook 1: Development and Implementation - RagAgent with PDF Parsing and Prechunking**\n",
    "## **Introduction:**\n",
    "\n",
    "In this notebook, we will build a Retrieval-Augmented Generation (RAG) agent that can retrieve insights from a PDF document. The system will combine PDF parsing, chunking, and document retrieval, allowing us to construct a robust pipeline for handling unstructured data. Weâ€™ll use the PDFtoTextParser for extracting content from PDFs, the SentenceChunker for splitting text into manageable chunks, and the RagAgent for retrieving relevant information based on queries.\n",
    "\n",
    "By the end of this notebook, youâ€™ll have implemented a solution capable of parsing, chunking, and retrieving information using an LLM-based agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Using cached groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\admin\\documents\\tech projects\\swarmauri\\swarmauri-0.5.0\\lib\\site-packages (from groq) (4.6.2.post1)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\documents\\tech projects\\swarmauri\\swarmauri-0.5.0\\lib\\site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\admin\\documents\\tech projects\\swarmauri\\swarmauri-0.5.0\\lib\\site-packages (from groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\documents\\tech projects\\swarmauri\\swarmauri-0.5.0\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\admin\\documents\\tech projects\\swarmauri\\swarmauri-0.5.0\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\admin\\documents\\tech projects\\swarmauri\\swarmauri-0.5.0\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\documents\\tech projects\\swarmauri\\swarmauri-0.5.0\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\documents\\tech projects\\swarmauri\\swarmauri-0.5.0\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\documents\\tech projects\\swarmauri\\swarmauri-0.5.0\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\documents\\tech projects\\swarmauri\\swarmauri-0.5.0\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\admin\\documents\\tech projects\\swarmauri\\swarmauri-0.5.0\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
      "Using cached groq-0.11.0-py3-none-any.whl (106 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, groq\n",
      "Successfully installed distro-1.9.0 groq-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import necessary modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from swarmauri.chunkers.concrete.SentenceChunker import SentenceChunker\n",
    "from swarmauri_community.parsers.concrete.FitzPdfParser import PDFtoTextParser as Parser\n",
    "from swarmauri.llms.concrete.GroqModel import GroqModel as LLM\n",
    "from swarmauri.vector_stores.concrete.TfidfVectorStore import TfidfVectorStore\n",
    "from swarmauri.agents.concrete.RagAgent import RagAgent\n",
    "from swarmauri.documents.concrete.Document import Document\n",
    "from swarmauri.messages.concrete.SystemMessage import SystemMessage\n",
    "from swarmauri.conversations.concrete.MaxSystemContextConversation import MaxSystemContextConversation\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load environment variables (ensure GROQ_API_KEY is set in .env)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize PDF parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(name=None, id='dbf3d94b-3890-4488-89a3-0c98d0da11a2', members=[], owner=None, host=None, resource='Document', version='0.1.0', type='Document', content='Engineering Applications of Artificial Intelligence 126 (2023) 107021\\nAvailable online 4 September 2023\\n0952-1976/Â© 2023 Elsevier Ltd. All rights reserved.\\nContents lists available at ScienceDirect\\nEngineering Applications of Artificial Intelligence\\njournal homepage: www.elsevier.com/locate/engappai\\nSurvey paper\\nTransformer for object detection: Review and benchmarkâœ©\\nYong Li a,âˆ—, Naipeng Miao a, Liangdi Ma b, Feng Shuang a, Xingwen Huang a\\na Guangxi Key Laboratory of Intelligent Control and Maintenance of Power Equipment, School of Electrical Engineering, Guangxi University, No. 100, Daxuedong\\nRoad, Xixiangtang District, Nanning, 530004, Guangxi, China\\nb School of Software, Tsinghua University, No. 30 Shuangqing Road, Haidian District, Beijing, 100084, China\\nA R T I C L E\\nI N F O\\nKeywords:\\nReview\\nObject detection\\nTransformer-based models\\nCOCO2017 dataset\\nBenchmark\\nA B S T R A C T\\nObject detection is a crucial task in computer vision (CV). With the rapid advancement of Transformer-\\nbased models in natural language processing (NLP) and various visual tasks, Transformer structures are\\nbecoming increasingly prevalent in CV tasks. In recent years, numerous Transformer-based object detectors\\nhave been proposed, achieving performance comparable to mainstream convolutional neural network-based\\n(CNN-based) approaches. To provide researchers with a comprehensive understanding of the development,\\nadvantages, disadvantages, and future potential of Transformer-based object detectors in Artificial Intelligence\\n(AI), this paper systematically reviews the mainstream methods and analyzes the limitations and challenges\\nencountered in their current applications, while also offering insights into future research directions. We have\\nreviewed a large number of papers, selected the most prominent Transformer detection methods, and divided\\nthem into Transformer Neck and Transformer Backbone categories for introduction and comparative analysis.\\nFurthermore, we have constructed a benchmark using the COCO2017 dataset to evaluate different object\\ndetection algorithms. Finally, we summarize the challenges and prospects in this field.\\n1. Introduction\\nObject detection is a fundamental task in computer vision that\\nrequires simultaneous classification and localization of potential objects\\nwithin a single image (Zhao et al., 2019). As such, it plays a cru-\\ncial role in various applications, including autonomous driving (Chen\\net al., 2015, 2017), face recognition (Sung and Poggio, 1998), pedes-\\ntrian detection (Dollar et al., 2012), and medical detection (Kobatake\\nand Yoshinaga, 1996). The performance of object detection directly\\ninfluences object tracking, environment perception, and scene under-\\nstanding (Felzenszwalb et al., 2010). Recently, deep learning-based\\nobject detection methods have gained considerable attention due to\\nthe rapid development of deep learning. However, numerous challenges\\nremain, such as balancing accuracy and efficiency, handling multi-scale\\nobjects, and creating lightweight models.\\nTraditional mainstream object detection methods have predomi-\\nnantly utilized convolutional neural networks (CNNs), including Faster\\nR-CNN (Ren et al., 2016), SSD (Liu et al., 2016), and YOLO with\\nits variants (Redmon et al., 2016; Redmon and Farhadi, 2018, 2017;\\nBochkovskiy et al., 2020; Ge et al., 2021). Owing to the remark-\\nable success of Transformers in natural language processing (NLP),\\nâœ©This work was supported by the Guangxi Science and Technology base and Talent Project (Grant No. Guike AD22080043), the Key Laboratory of\\nAdvanced Manufacturing Technology, Ministry of Education (Grant No. GZUAMT2021KF04), and the National Natural Science Foundation of China (Grant No.\\n61720106009).\\nâˆ—Corresponding author.\\nE-mail address: yongli@gxu.edu.cn (Y. Li).\\nresearchers have endeavored to adapt Transformer architectures for\\ncomputer vision tasks. As a result, numerous Transformer-based vision\\nmodels have emerged in recent years, achieving performance levels that\\nare comparable or even superior to their CNN counterparts.\\nTransformer (Vaswani et al., 2017) was initially proposed as an\\narchitecture based on the self-attention mechanism for machine trans-\\nlation and sequence modeling tasks (Sutskever et al., 2014). In recent\\nyears, Transformer has experienced significant advancements in NLP\\nand has become a mainstream deep learning model, such as BERT (De-\\nvlin et al., 2018) and its variants (Lan et al., 2019; Liu et al., 2019), GPT\\nseries (Radford et al., 2018, 2019; Brown et al., 2020), and others. Due\\nto its scalability, Transformer can be pre-trained on large datasets and\\nsubsequently fine-tuned for downstream tasks.\\nTransformers in object detection have garnered increasing attention,\\nparticularly over the last three years. Several high-performance models\\nhave been proposed, such as DETR (Carion et al., 2020), Deformable\\nDETR (Dai et al., 2017), Swin Transformer (Liu et al., 2021b,a),\\nDINO (Zhang et al., 2022a), and more. Currently, Transformer-based\\nmodels have emerged as a new paradigm in object detection, making a\\nsystematic analysis and evaluation of numerous existing Transformer-\\nbased detectors essential for future research.\\nhttps://doi.org/10.1016/j.engappai.2023.107021\\nReceived 22 October 2022; Received in revised form 25 May 2023; Accepted 19 August 2023\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n2\\nY. Li et al.\\nFig. 1. Chronological overview of most Transformer-based object detection methods.\\nSome reviews (Khan et al., 2021; Liu et al., 2021c; Arkin et al.,\\n2021; Han et al., 2022; Arkin et al., 2022) have provided detailed\\nintroductions and analyses of Transformer-based detectors. In contrast\\nto these surveys, our study not only presents a thorough comparison\\nof the strengths and weaknesses of object detectors based on both\\nTransformer and CNN architectures, but also classifies the prevalent\\nTransformer-based detectors into Transformer Backbone and Trans-\\nformer Neck categories. Moreover, we systematically analyze their\\nperformance, potential, and limitations. We investigate the advance-\\nments and constraints of various state-of-the-art Transformer-based\\ndetectors (Table 6) and establish benchmarks for these methods using\\nthe COCO2017 dataset (Tables. 4, 5). We hope this review delivers\\na comprehensive understanding of Transformer-based object detectors\\nfor researchers.\\nWe have categorized existing methods into two groups based on\\nthe role of Transformer in the overall model architecture: Transformer\\nNeck and Transformer Backbone, as illustrated in Fig. 1. We present\\na detailed analysis of representative methods, compare these methods\\nhorizontally on the COCO2017 dataset (Lin et al., 2014), and summa-\\nrize the novelty of each method, such as Transformer Backbone with\\nhierarchical representation structure, spatial prior acceleration based\\non sparse attention, and pure sequence processing for object detection,\\namong others. The main contributions of this paper are as follows:\\n1. We provide a comprehensive summary of state-of-the-art Trans-\\nformer-based object detectors from the past three years, high-\\nlighting recent breakthroughs in Transformer architecture for\\nobject detection. For each representative model, we offer an in-\\ndepth analysis while examining its relationship and connections\\nwith other models, both incrementally and comparatively. More-\\nover, we compare the strengths and weaknesses of Transformer\\nand CNN architectures, and further discuss the performance, key\\nfeatures, and limitations of both Transformer Neck (DETR-like\\nmodels) and Transformer Backbone (ViT-like models).\\n2. We comprehensively compare mainstream models on the same\\ndataset, establish a benchmark based on the COCO2017 dataset,\\nand offer insightful discussions.\\n3. We present an in-depth analysis of the transition as Transformer\\narchitecture extends from sequence to visual tasks. Furthermore,\\nwe discuss the future development of Transformer and CNN\\napproaches in object detection.\\nThe rest structure of this paper is organized as follows. Section 2\\nintroduces the main object detection datasets and evaluation metrics,\\nas well as the Attention mechanism and Transformer basic architecture.\\nSection 3 outlines the current mainstream Transformer-based object de-\\ntectors. Section 4 discusses the methods of these models in a multi-level\\ncomparison. Section 5 concludes the paper with an outlook.\\n2. Transformer architecture\\nTransformer is an architecture based on the attention mechanism\\nproposed by Vaswani et al. (2017) in 2017, which was initially used\\nfor machine translation tasks and subsequently achieved great success\\nin NLP (Devlin et al., 2018). The success of Transformer is attributed\\nFig. 2. Transformer structure. The Input Embedding module of Transformer encoder\\n(left column) can map the input sequence to Embedding space and pass it to encoder\\nmodule for processing. The Transformer decoder (right column) receives the previous\\noutput sequence and the output sequence from the intermediate encoder. The previous\\noutput sequence will be shifted one bit to the right, and the start token will be appended\\nbefore the sequence to get the input from the decoder. The feed-forward network and\\nthe multi-head attention module are repeated ğ‘times to form the encoder and decoder.\\nto its unique architecture, whose core design is the Encoderâ€“Decoder\\nstructure based on self-attention. As shown in Fig. 2, Transformer con-\\nsists of three main blocks: multi-headed attention, positional encoding,\\nand feed-forward network. Multi-head attention (MHA) block and Feed-\\nforward network block are the main modules of Encoder and Decoder.\\nPosition encoding is a vital module to all Transformer variants and is\\nresponsible for attaching position information to the input sequence. In\\nthis section, these fundamental techniques are described in detail.\\n2.1. Basic architecture\\nThe structure of Transformer is based on encoderâ€“decoder. The\\nencoder consists of ğ‘basic encoder modules, as shown in Fig. 2. Every\\nencoder module consists of a multi-head attention module (MHA) and a\\nfeed-forward network (FFN). And then, they are cascaded with residual\\nconnection and layer normalization one by one. Finally, the output of\\nthe encoder module is shown in Eq. (1):\\nğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡= ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿğ‘ğ‘œğ‘Ÿğ‘š(ğ‘¥+ ğ‘†ğ‘¢ğ‘ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿ(ğ‘¥)),\\n(1)\\nwhere ğ‘¥is the input sequence, and ğ‘†ğ‘¢ğ‘ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿrepresents the attention\\nmodule or feedforward network.\\n2.2. Self-attention\\n2.2.1. Scaled dot-product attention\\nThe self-attention mechanism module, as the core component of\\nTransformer, consists of two main parts: (1) Linear projection layer:\\nthe input sequence is mapped into 3 different vectors (query ğ‘„, key\\nğ¾, value ğ‘‰). The input sequences are ğ‘‹âˆˆRğ‘›ğ‘¥Ã—ğ‘‘ğ‘¥and ğ‘ŒâˆˆRğ‘›ğ‘¦Ã—ğ‘‘ğ‘¦,\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n3\\nY. Li et al.\\nFig. 3. (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of\\nseveral attention layers running in parallel.\\nwhere ğ‘›and ğ‘‘denote the length and dimension of the input sequence,\\nrespectively. Then ğ‘„, ğ¾, and ğ‘‰are generated as follows:\\nğ‘„= ğ‘‹ğ‘Šğ‘„,\\nğ¾= ğ‘Œğ‘Šğ¾,\\nğ‘‰= ğ‘Œğ‘Šğ‘‰,\\n(2)\\nwhere ğ‘Šğ‘„âˆˆRğ‘‘ğ‘¥Ã—ğ‘‘ğ‘, ğ‘Šğ¾âˆˆRğ‘‘ğ‘¦Ã—ğ‘‘ğ‘˜and ğ‘Šğ‘‰âˆˆRğ‘‘ğ‘¦Ã—ğ‘‘ğ‘£are the learnable\\nweight matrices. The ğ‘‘ğ‘and ğ‘‘ğ‘˜denotes the dimensions of ğ‘Šğ‘„and ğ‘Šğ¾,\\nrespectively. The dimension of ğ‘Šğ‘‰is ğ‘‘ğ‘£. When ğ‘‹= ğ‘Œ, Eq. (2) is the\\nself-attention computation, and when ğ‘‹â‰ ğ‘Œ, it is the cross-attention\\ncomputation in the Decoder module.\\n(2) Attention layer: Transformer adopts a special attention method\\ncalled Scaled Dot-Product Attention, as shown in Fig. 3 (left). The\\ninput consists of ğ‘„in ğ‘‘ğ‘dimensions, ğ¾in ğ‘‘ğ‘˜dimensions and ğ‘‰in\\nğ‘‘ğ‘£dimensions, and the scaled attention matrix is calculated as shown\\nin Eq. (3).\\nAttention (ğ‘„, ğ¾, ğ‘‰) = softmax\\n(\\nğ‘„ğ¾ğ‘‡\\nâˆš\\nğ‘‘ğ‘˜\\n)\\nğ‘‰,\\n(3)\\nwhere\\n1\\nâˆš\\nğ‘‘ğ‘˜is the scaling factor. The attention weights are obtained by\\ncomputing the dot product of Q for all K. The attention weights are then\\nnormalized by the scaling factor\\n1\\nâˆš\\nğ‘‘ğ‘˜and the softmax layer. The output\\nweights are assigned to the corresponding elements of V to obtain the\\nfinal attention matrix.\\n2.2.2. Multi-head attention\\nHowever, the modeling ability of single-head attention is weak.\\nTo address this problem, Vaswani et al. (2017) proposed multi-head\\nattention (MHA). The structure is shown in Fig. 3 (right). MHA can\\nenhance the modeling ability of each attention layer without changing\\nthe number of parameters.\\nCompared to single-head attention, MHA maps Q, K, and V linearly\\nto different dimensional subspaces (ğ‘‘ğ‘, ğ‘‘ğ‘˜, ğ‘‘ğ‘£) to compute similarity and\\ncompute the attention function in parallel. As shown in Eq. (4), the\\nresulting vectors are concatenated and mapped again to obtain the final\\noutput.\\nMultiHead(ğ‘„, ğ¾, ğ‘‰) = Concat ( head 1, â€¦ , head h\\n) ğ‘Šğ‘‚,\\nwhere head i = Attention\\n(\\nğ‘„ğ‘Šğ‘„\\nğ‘–, ğ¾ğ‘Šğ¾\\nğ‘–, ğ‘‰ğ‘Šğ‘‰\\nğ‘–\\n)\\n,\\n(4)\\nwhere ğ‘Šğ‘„\\nğ‘–\\nâˆˆRğ‘‘model Ã—ğ‘‘ğ‘, ğ‘Šğ¾\\nğ‘–\\nâˆˆRğ‘‘model Ã—ğ‘‘ğ‘˜, ğ‘Šğ‘‰\\nğ‘–\\nâˆˆRğ‘‘model Ã— ğ‘‘ğ‘£, ğ‘Šğ‘‚\\nğ‘–\\nâˆˆ\\nRğ‘‘model Ã—â„ğ‘‘ğ‘£is the projection parameter matrix. Multi-head attention\\nreduces the dimensionality of each vector when calculating the atten-\\ntion of each head, which reduces overfitting to a certain extent. Since\\nattention has different distributions in different subspaces, this module\\nfuses the feature relationships between different sequence dimensions\\nin vector concatenation.\\n2.3. Position-wise feed-forward networks\\nThe output of the MHA layer is fed into the feed-forward network\\n(FFN). FFN is mainly composed of two linear transformations with a\\nRuLU activation in between. The output of FFN can be expressed as\\nshown in Eq. (5):\\nFFN(ğ‘¥) = max (0, ğ‘¥ğ‘Š1 + ğ‘1\\n) ğ‘Š2 + ğ‘2,\\n(5)\\nwhere ğ‘Š1 and ğ‘Š2 denote weight matrices of the two fully connected\\nlayers.\\n2.4. Positional encoding\\nUnlike CNN and RNN, self-attention computation brings the ad-\\nvantage of parallel computing while losing word order information.\\nTherefore, positional encoding is used to provide positional information\\nto the model. In detail, a position-dependent signal is added to each\\nword embedding for each input sequence to help the model incorporate\\nthe order of words. The output of positional encoding has the same\\ndimension as the embedding layer. So it can be superimposed directly\\non Embedding. The positional information of each token (a sequence\\nof primitives obtained after the text has been divided into words) and\\nits semantic information (Embedding) are fully integrated and passed\\nto the subsequent layer.\\nThere are many variants of positional encoding. The original Trans-\\nformer uses sine and cosine functions for positional encoding, as shown\\nin Eq. (6).\\nğ‘ƒğ¸(ğ‘ğ‘œğ‘ ,2ğ‘–) = sin (ğ‘ğ‘œğ‘ âˆ•100002ğ‘–âˆ•ğ‘‘model ) ,\\nğ‘ƒğ¸(ğ‘ğ‘œğ‘ ,2ğ‘–+1) = cos (ğ‘ğ‘œğ‘ âˆ•100002ğ‘–âˆ•ğ‘‘model ) ,\\n(6)\\nwhere ğ‘ğ‘œğ‘ is the position and ğ‘–is the dimension. That is, each dimension\\nof the position encoding corresponds to a sine wave. The wavelengths\\nform a geometric progression from 2ğœ‹to 10000Ã—2ğœ‹. For any fixed offset\\nğ‘˜, ğ‘ƒğ¸ğ‘ğ‘œğ‘ +ğ‘˜can be represented as a linear function of ğ‘ƒğ¸ğ‘ğ‘œğ‘ .\\n2.5. Remarks\\nThe self-attention mechanism allows Transformer to break through\\nthe limitation that RNN models cannot be computed in parallel and\\nimprove the computational efficiency. Compared with CNN, the self-\\nattentive mechanism has a global perceptual field. The number of\\noperations required to compute the association between two locations\\ndoes not grow with distance, so it has a stronger ability to learn long-\\nrange dependencies. In addition, Transformer has a general modeling\\ncapability. Transformer can be regarded as a fully connected graph\\nmodeling method that can model heterogeneous nodes by projecting\\nthem into a comparable space to compute similarity. Therefore, there is\\na sufficient theoretical basis for using Transformer for various computer\\nvision tasks based on its general modeling capability. Considering\\nthe dimensional differences between images and text, the images are\\nconverted into sequences and can then be input into the model for\\nprocessing.\\nMoreover, we compare the characteristics of CNN and Transformer.\\nAs shown in Table 1, Transformer tends to model shapes more but\\nrequires massive data for training. In contrast, CNN tends to model local\\ntextures more but has to pile many convolutional layers to have a large\\nenough receptive field to get global information (Geirhos et al., 2019).\\n3. Transformer for object detection\\nThis section first introduces common datasets and evaluation met-\\nrics for object detection and analyzes classic Transformer-based object\\ndetectors. According to their structural difference, We classify the\\nlisted detectors as Transformer Neck-based detectors and Transformer\\nBackbone-based detectors. The Transformer Neck-based detector infers\\nthe class labels and bounding box coordinates with a set of learnable\\nobject queries but does not change the backbone used for feature\\nextraction. Transformer Backbone-based detectors propose a generic\\nvisual backbone that flattens the image into a sequence instead of\\nconvolution for feature extraction. Multiscale feature fusion is also\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n4\\nY. Li et al.\\nTable 1\\nSummary of the highlights and limitations of CNN and Transformer.\\nArchitecture\\nHighlights\\nLimitations\\nTransformer\\n(1) The attention mechanism\\namplifies the significance of crucial\\naspects of an image while reducing\\nthe rest, thereby concentrating on\\nmore relevant features. This\\nmechanism assists the Transformer in\\nmodeling the long-range\\ndependencies of input sequence\\nelements and thus enhances its\\ngeneralization ability for samples\\noutside the distribution (Bai et al.,\\n2021). (2) Unlike methods such as\\nRNN and LSTM, the Transformer\\nallows for parallel computations. (3)\\nGiven its straightforward yet\\nadaptable design, the Transformer\\ncan tackle multiple tasks\\nsimultaneously, rendering it a\\npotential candidate for a\\ngeneral-purpose model handling\\nvarious tasks.\\n(1) Transformer-based models are\\nknown for their substantial data\\nrequirements and computationally\\nexpensive nature, particularly when\\napplied to vision tasks (He et al.,\\n2021). (2) They are also\\ncharacterized by a slower rate of\\nconvergence, which can pose\\nchallenges in their utilization (Gao\\net al., 2021). (3) Further, these\\nmodels often involve high\\ncomputational overhead, which\\nexacerbates their deployment issues\\nin resource-constrained settings (Li\\net al., 2022).\\nCNN\\n(1) CNN-based models have strong\\nlocal feature extraction ability\\nbenefited from inductive bias\\nproperties such as translation\\ninvariance, weight sharing, and\\nsparse connectivity. (2) CNNs can\\noperate in parallel with lower\\ncomputational complexity than\\nTransformer.\\n(1) CNN rarely encodes relative\\nfeature positions, instead favoring\\nreceptive field expansion via larger\\nkernels or stacked layers, often\\nreducing local convolutionâ€™s\\ncomputational and statistical\\nefficiency. (2) CNNâ€™s global feature\\ncapture is comparatively weaker than\\nTransformer models (Liu et al.,\\n2021b).\\nTable 2\\nBriefing on datasets for object detection.\\nName\\nImage volume\\nclass\\nSource\\nAnnotation format\\nVOC2007\\n9963\\n20\\nPASCAL\\nXML\\nVOC2012\\n17112\\n20\\nPASCAL\\nXML\\nCOCO2017\\n121408\\n80\\nMicrosoft\\nJSON\\nincorporated in many methods to improve detection accuracy and\\nreplace the CNN backbone in classical detectors. In reviewing these\\nmethods, we summarize the optimization innovations or modules of the\\ndifferent methods. Finally, we compare their performance in Table 4\\nand Table 5 and give analyzation and discussion on improvements of\\nthe above methods.\\n3.1. Common datasets and evaluation metrics\\n3.1.1. Common datasets for object detection\\nDatasets are the basis for measuring and comparing algorithm\\nperformance. The commonly used object detection datasets are Pas-\\ncal VOC2007(Everingham et al., 2007), Pascal VOC2012(Everingham\\net al., 2012) and Microsoft COCO2017(Lin et al., 2014), as shown in\\nTable 2. The Pascal VOC dataset has only 20 object categories and is\\nregarded as a benchmark dataset for object detection. Compared with\\nVOC, the COCO dataset has more small objects and more objects in\\na single image, and most of the objects are non-centrally distributed\\nand more similar to the real environment. Thus COCO dataset is\\nmore difficult for object detection and has been the mainstream object\\ndetection dataset in recent years.\\n3.1.2. Evaluation metrics\\nCommon evaluation metrics for object detection include Precision,\\nRecall, Average Precision (AP), and mean Average Precision (mAP). In\\naddition to classification, the object detection task localizes the object\\nfurther with a bounding box associated with its corresponding confi-\\ndence score to report how certain the bounding box of the object class\\nis detected. Therefore to determine how many objects were detected\\ncorrectly and how many false positives were generated, we use the\\nIntersection over Union (IoU) metric.\\nIntersection over Union (IoU). IoU is an evaluation metric that\\nquantifies the similarity between the ground truth bounding box\\n(ğ‘”ğ‘¡ğ‘ğ‘œğ‘¥) and the predicted bounding box (ğ‘ğ‘‘ğ‘ğ‘œğ‘¥) to evaluate how good\\nthe predicted box is. The IoU score ranges from 0 to 1; the closer the\\ntwo boxes, the higher the IoU score. It can be calculated as follow:\\nğ¼ğ‘œğ‘ˆ(ğ‘”ğ‘¡, ğ‘ğ‘‘) = area(ğ‘”ğ‘¡ğ‘ğ‘œğ‘¥âˆ©ğ‘ğ‘‘ğ‘ğ‘œğ‘¥)\\narea(ğ‘”ğ‘¡ğ‘ğ‘œğ‘¥âˆªğ‘ğ‘‘ğ‘ğ‘œğ‘¥) ,\\n(7)\\nFor the IoU threshold at ğ›¼, True Positive(TP) is a detection for\\nwhich ğ¼ğ‘œğ‘ˆ(ğ‘”ğ‘¡, ğ‘ğ‘‘) â‰¥ğ›¼and False Positive (FP) is a detection for which\\nğ¼ğ‘œğ‘ˆ(ğ‘”ğ‘¡, ğ‘ğ‘‘) â‰¤ğ›¼. False Negative (FN) is a ground-truth missed together\\nwith ğ‘”ğ‘¡for which ğ¼ğ‘œğ‘ˆ(ğ‘”ğ‘¡, ğ‘ğ‘‘) â‰¤ğ›¼. The definitions of TP, TN, FP and FN\\nare shown in Table 3.\\nPrecision. Precision is the probability of the predicted bounding\\nboxes matching actual ground truth boxes, also referred to as the\\npositive predictive value. Precision scores range from 0 to 1, with a\\nhigh precision implying that most detected objects match ground truth\\nobjects.\\nRecall. Recall is the true positive rate, also referred to as sensitivity,\\nwhich measures the probability of ground truth objects being correctly\\ndetected. Similarly, Recall ranges from 0 to 1, where a high recall score\\nmeans that most ground truth objects were detected.\\nThe Precision and Recall can be calculated as follow:\\nPrecision =\\nTP\\nTP + FP ,\\n(8)\\nRecall =\\nTP\\nTP + FN ,\\n(9)\\nAverage precision (AP). AP is Area Under the Precisionâ€“Recall\\nCurve evaluated at a specific IoU threshold. AP is a single number\\nmetric that combines precision and recall and describes the Accuracyâ€“\\nRecall curve by AP among recall values ranging from 0 to 1. It is used\\nto evaluate the performance of object detectors.\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n5\\nY. Li et al.\\nTable 3\\nDefinition of terms.\\nTerms\\nDefinitions\\nTP (True Positive)\\nPositive samples are correctly identified as positive samples.\\nTN (True Negative)\\nNegative samples are correctly identified as negative samples.\\nFP (False Positive)\\nFalse positive samples, that is, negative samples are mistakenly identified as positive samples.\\nFN (False Negative)\\nFalse negative samples, that is, positive samples are wrongly identified as negative samples.\\nFig. 4. The pipeline of DETR. The backbone is a convolutional neural network (CNN) that serves as a feature extractor. And Transformer is the core of the DETR architecture,\\nconsisting of an encoder and a decoder. The high-dimensional feature map from the backbone is flattened and fed into the encoder. Then encoder processes the spatial information\\nand outputs a sequence of encoded feature vectors. Finally, The output of the decoder is passed through a series of linear layers to predict the final bounding box coordinates and\\nclass probabilities for each object query.\\nSource: Image from Carion et al. (2020).\\nMean average precision (mAP). AP is calculated for each class\\nindividually, and mAP is the average of AP values across all classes.\\nThe mAP can be calculated as Eq. (10). There are two kinds of mAPs\\ncommonly used. (1) PASCAL VOC challenge uses mAP as a metric\\nwith an IoU threshold of 0.5. (2) While MS COCO averages mAP over\\ndifferent IoU thresholds 50% to 95% with a step of 0.05, this metric\\nis denoted in papers by mAP@[.5,.95]. Therefore, COCO not only\\naverages AP over all classes but also on the defined IoU thresholds.\\nmAP =\\nâˆ‘ğ‘˜\\nğ‘–=1 APğ‘–\\nğ‘˜\\nfor ğ‘˜classes,\\n(10)\\nFrame Per Second (FPS). FPS defines how fast your object detec-\\ntion model processes your video and generates the desired output.\\n3.2. Transformer neck\\nIn this section, we review the classic Transformer Neck-based ob-\\nject detection models in last two years, starting from the original\\nTransformer detector DETR (Carion et al., 2020). The original DETR\\nregards object detection as end-to-end set prediction, thus removing\\nhand-designed components such as anchor boxes and non-maximum\\nsuppression (NMS). However, some drawbacks need to be solved in\\nDETR, such as slow convergence and poor detection of small objects.\\nTherefore, many approaches (sparse attention, spatial prior acceler-\\nation, multi-scale detection) have been proposed to improve it by\\nresearchers. We compare the performance of all methods together on\\nthe COCO2017 dataset with the benchmark shown in Table 4.\\n3.2.1. DETR\\nDETR proposed by Carion et al. (2020) is the first object detector\\nthat successfully uses the Transformer as the main module in object\\ndetection. DETR not only has a simpler and more flexible structure\\nbut also has comparable performance compared to previous SOTA\\napproaches, such as the highly optimized Faster R-CNN. Unlike classical\\nobject detectors, DETR is an end-to-end object detection model. It gets\\nrid of the autoregressive model, performs parallel inference on object\\nrelationships and global image context, and then outputs the final\\npredictions. The structure of DETR is shown in Fig. 4.\\nDETR treats the object detection task as an intuitive set prediction\\nproblem and discards some traditional hand-craft components such as\\nhand-designed anchor sets and non-maximal suppression (NMS). As\\nshown in Fig. 4, DETR uses CNN Backbone to learn the 2D features\\nof the input image. Then feature maps are unfolded into sequences and\\nfed to the Transformer encoder module (where there is still positional\\nencoding). The output of the Transformer Decoder module is then\\nobtained under the constraint of object queries. Finally, the class and\\nbounding box regression parameters are obtained after a feedforward\\nnetwork.\\nBased on the idea of sequential prediction, DETR regards the predic-\\ntion of the network as a fixed sequence Ìƒğ‘¦of length N, Ìƒğ‘¦= Ìƒğ‘¦ğ‘–, ğ‘–âˆˆ(1, ğ‘),\\n(where the value of ğ‘is fixed and much larger than the number of\\nGround Truth in the image), Ìƒğ‘¦ğ‘–= (Ìƒğ‘ğ‘–, Ìƒğ‘ğ‘–\\n). Meanwhile, the Ground Truth\\nis considered as a sequence ğ‘¦âˆ¶ğ‘¦ğ‘–= (ğ‘ğ‘–, ğ‘ğ‘–\\n) (the length must be less\\nthan N, so the sequence is filled with ğœ™(for no object), which can be\\ninterpreted as the category of background, to make its length equal to\\nN), where ğ‘ğ‘–denotes the true category to which the object belongs,\\nand ğ‘ğ‘–denotes a quaternion (containing the center point coordinates\\nand the width and height of the object box, and both are relative to\\nthe scale coordinates of the image).\\nSo the prediction task can be viewed as a bipartite matching prob-\\nlem between ğ‘¦and Ìƒğ‘¦, with the Hungarian algorithm as the solution\\nmethod, defining the strategy for minimum matching as follows:\\nÌ‚ğœ= arg min\\nğœâˆˆSğ‘\\nğ‘\\nâˆ‘\\nğ‘–\\nLmatch\\n(ğ‘¦ğ‘–, Ì‚ğ‘¦ğœ(ğ‘–)\\n) ,\\n(11)\\nwhere Ìƒğœdenotes the matching strategy when finding the minimum\\nloss, for L while considering the similarity prediction between Ground\\nTruth boxes. For ğœ(ğ‘–), ğ‘ğ‘–the predicted category confidence is Ìƒğ‘ƒğœ(ğ‘–)\\n(ğ‘ğ‘–\\n)\\nand the bounding box prediction is Ìƒğ‘ğœ(ğ‘–), for non-empty matches, define\\nLmatch\\n(ğ‘¦ğ‘–, Ì‚ğ‘¦ğœ(ğ‘–)\\n) as: âˆ’1{ğ‘ğ‘–â‰ âˆ…} Ì‚ğ‘ğœ(ğ‘–)\\n(ğ‘ğ‘–\\n) + 1{ğ‘ğ‘–â‰ âˆ…}Lbox\\n(ğ‘ğ‘–, Ì‚ğ‘ğœ(ğ‘–)\\n).\\nIn this way, the overall loss is obtained as\\nLHungarian (ğ‘¦, Ì‚ğ‘¦) =\\nğ‘\\nâˆ‘\\nğ‘–=1\\n[\\nâˆ’log Ì‚ğ‘Ì‚ğœ(ğ‘–)\\n(ğ‘ğ‘–\\n) + 1{ğ‘ğ‘–â‰ âˆ…}Lbox\\n(ğ‘ğ‘–, Ì‚ğ‘Ì‚ğœ(ğ‘–))]\\n,\\n(12)\\nConsidering the bounding box scale, the ğ¿1 loss and the IoU loss are\\nlinearly combined to obtain the Lğ‘ğ‘œğ‘¥loss:\\nLğ‘ğ‘œğ‘¥= ğœ†iou Liou\\n(ğ‘ğ‘–, Ì‚ğ‘ğœ(ğ‘–)\\n) + ğœ†L1 â€–â€–â€–ğ‘ğ‘–âˆ’Ì‚ğ‘ğœ(ğ‘–)â€–â€–â€–1 ,\\n(13)\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n6\\nY. Li et al.\\nTable 4\\nComparison between Transformer Necks and representative CNNs on COCO2017 Val set. â€˜â€˜Multi-Scaleâ€™â€™ refers to multi-scale inputs. AP denotes IoU threshold = .50:.05:.95. AP50\\nand AP75 denote IoU threshold = .50 and .75. In addition, APğ‘†, APğ‘€, APğ¿denote different scales of objects. Small means area < 3232. Medium means 3232 < area < 9696. Large\\nmeans area > 9696.\\nMethod\\nBackbone\\nEpochs\\nGFLOPs\\n#Params(M)\\nMulti-scale\\nFPS\\nAP\\nğ€ğ50\\nğ€ğ75\\nğ€ğğ‘†\\nğ€ğğ‘€\\nğ€ğğ¿\\nFaster R-CNN + FPN (Ren et al., 2016)\\nResNet50\\n109\\n180\\n42\\nâ€“\\n26\\n42.0\\n62.1\\n45.5\\n26.6\\n45.4\\n53.4\\nDERR+ (Carion et al., 2020)\\nResNet50\\n500\\n86\\n41\\nâ€“\\n28\\n42.0\\n62.4\\n44.2\\n20.5\\n45.8\\n61.1\\nDETR-DC5+ (Carion et al., 2020)\\n500\\n187\\n41\\nâ€“\\n12\\n43.4\\n63.1\\n45.9\\n22.5\\n47.3\\n61.1\\nDERR (Carion et al., 2020)\\n50\\n86\\n41\\nâ€“\\n12\\n42.0\\n62.4\\n44.2\\n20.5\\n45.8\\n61.1\\nDETR-DC5 (Carion et al., 2020)\\n50\\n187\\n41\\nâ€“\\n12\\n43.4\\n63.1\\n45.9\\n22.5\\n47.3\\n61.1\\nUP-DETR (Dai et al., 2021a)\\nResNet50\\n150\\n86\\n41\\nâ€“\\n28\\n40.5\\n60.8\\n42.6\\n19.0\\n44.4\\n60.0\\nUP-DETR+ (Dai et al., 2021a)\\n300\\n86\\n41\\nâ€“\\n28\\n42.8\\n63.0\\n45.3\\n20.8\\n47.1\\n61.7\\nDeformable DETR (Zhu et al., 2021)\\nResNet50\\n50\\n173\\n40\\nâ€“\\n19\\n43.8\\n62.6\\n47.7\\n26.4\\n47.1\\n58.0\\nTwo-stage Deformable DETR (Zhu et al., 2021)\\n50\\n173\\n40\\nâ€“\\n19\\n46.2\\n65.2\\n50.0\\n28.8\\n49.2\\n61.7\\nConditional DETR (Meng et al., 2021)\\nResNet50\\n108\\n90\\n44\\nâ€“\\nâ€“\\n43.0\\n64.0\\n45.7\\n22.7\\n46.7\\n61.5\\nConditional DETR-DC5 (Meng et al., 2021)\\n108\\n195\\n44\\nâ€“\\nâ€“\\n45.1\\n65.4\\n48.5\\n25.3\\n49.0\\n62.2\\nACT-MTKD(L=16) (Zheng et al., 2021)\\nResNet50\\nâ€“\\n156\\nâ€“\\nâ€“\\n14\\n40.6\\nâ€“\\nâ€“\\n18.5\\n44.3\\n59.7\\nACT-MTKD(L=32) (Zheng et al., 2021)\\nâ€“\\n169\\nâ€“\\nâ€“\\n16\\n43.1\\nâ€“\\nâ€“\\n22.2\\n47.1\\n61.4\\nSMCA (Gao et al., 2021)\\nResNet50\\n50\\n152\\n40\\nâ€“\\n10\\n43.7\\n63.6\\n47.2\\n24.2\\n47.0\\n60.4\\nSMCA+ (Gao et al., 2021)\\n50\\n152\\n108\\nâ€“\\n10\\n45.6\\n65.5\\n49.1\\n25.9\\n49.3\\n62.6\\nEfficient DETR (Yao et al., 2021)\\nResNet50\\n36\\n159\\n32\\nâ€“\\nâ€“\\n44.2\\n62.2\\n48.0\\n28.4\\n47.5\\n56.6\\nEfficient DETR* (Yao et al., 2021)\\n36\\n210\\n35\\nâ€“\\nâ€“\\n45.1\\n65.4\\n48.5\\n25.3\\n49.0\\n62.2\\nTSP-FCOS (Sun et al., 2021)\\nResNet50\\n36\\n189\\n51.5\\nâ€“\\n15\\n43.1\\n62.3\\n47.0\\n26.6\\n46.8\\n55.9\\nTSP-RCNN (Sun et al., 2021)\\n36\\n188\\n64\\nâ€“\\n11\\n43.8\\n63.3\\n48.3\\n28.6\\n46.9\\n55.7\\nTSP-RCNN+ (Sun et al., 2021)\\n96\\n188\\n64\\nâ€“\\n11\\n45.0\\n64.5\\n49.6\\n29.7\\n47.7\\n58.0\\nYOLOS-S (Fang et al., 2021)\\nDeiT-S\\n150\\n200\\n30.7\\nâ€“\\n7\\n36.1\\n56.4\\n37.1\\n15.3\\n38.5\\n56.1\\nYOLOS-S (Fang et al., 2021)\\n150\\n179\\n27.9\\nâ€“\\n5\\n37.6\\n57.6\\n39.2\\n15.9\\n40.2\\n57.3\\nYOLOS-B (Fang et al., 2021)\\nDeiT-B\\n150\\n537\\n127\\nâ€“\\nâ€“\\n42.0\\n62.2\\n44.5\\n19.5\\n45.3\\n62.1\\nPnP-DETR-R50-DC5-ğ›¼-0.33 (Wang et al., 2021b)\\nResNet50\\n500\\n20.7(omit backbone)\\nâ€“\\nâ€“\\nâ€“\\n42.7\\n62.8\\n45.1\\n22.4\\n46.2\\n60.0\\nPnP-DETR-R50-DC5-ğ›¼-0.5 (Wang et al., 2021b)\\n500\\n32.9(omit backbone)\\nâ€“\\nâ€“\\nâ€“\\n43.1\\n63.4\\n45.3\\n22.7\\n46.5\\n61.1\\nDynamic DETR (Dai et al., 2021c)\\nResNet50\\n40\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\n47.2\\n65.9\\n51.1\\n28.6\\n49.3\\n59.1\\nAnchor DETR-C5 (Wang et al., 2021c)\\nResNet50\\n50\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\n42.1\\n63.1\\n44.9\\n22.3\\n46.2\\n60.0\\nAnchor DETR-DC5 (Wang et al., 2021c)\\n50\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\n44.2\\n64.7\\n47.5\\n24.7\\n48.2\\n60.6\\nD2ETR (Lin et al., 2022)\\nPVT2\\n50\\n82\\n35\\nâ€“\\nâ€“\\n43.2\\n62.9\\n46.2\\n22.0\\n48.5\\n62.4\\nDeformable D2ETR (Lin et al., 2022)\\n50\\n93\\n40\\nâ€“\\nâ€“\\n50.0\\n67.9\\n54.1\\n31.7\\n53.4\\n66.7\\nSparse DETR- ğœŒ= 10% (Roh et al., 2022)\\nResNet50\\n50\\n105\\n41\\nâ€“\\n25.3\\n45.3\\n65.8\\n49.3\\n28.4\\n48.3\\n60.1\\nSparse DETR- ğœŒ= 10% (Roh et al., 2022)\\nSwin-T\\n50\\n113\\n41\\nâ€“\\n21.2\\n48.2\\n69.2\\n52.3\\n29.8\\n51.2\\n64.5\\nDAB-DETR (Liu et al., 2022a)\\nResNet50\\n50\\n202\\n44\\nâ€“\\nâ€“\\n44.5\\n65.1\\n47.7\\n25.3\\n48.2\\n62.3\\nDAB-DETR* (Liu et al., 2022a)\\n50\\n216\\n44\\nâ€“\\nâ€“\\n45.7\\n66.2\\n49.0\\n26.1\\n49.4\\n63.1\\nDN-DETR (Li et al., 2022)\\nResNet50\\n50\\n94\\n44\\nâ€“\\nâ€“\\n44.1\\n64.4\\n46.7\\n22.9\\n48.0\\n63.4\\nDN-DETR-DC5 (Li et al., 2022)\\n50\\n202\\n44\\nâ€“\\nâ€“\\n46.3\\n66.4\\n49.7\\n26.7\\n50.0\\n64.3\\nDN-Deformable-DETR (Li et al., 2022)\\n50\\n195\\n48\\nâ€“\\nâ€“\\n48.6\\n67.4\\n52.7\\n31.0\\n52.0\\n63.7\\nDINO-4scale (Zhang et al., 2022a)\\nResNet50\\n12\\n279\\n47\\nâ€“\\n24\\n47.9\\n65.3\\n52.1\\n31.2\\n50.9\\n61.9\\nDINO-5scale (Zhang et al., 2022a)\\n12\\n860\\n47\\nâ€“\\n10\\n48.3\\n65.8\\n52.4\\n32.2\\n51.3\\n62.2\\nDINO-4scale (Zhang et al., 2022a)\\n36\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\n50.5\\n68.3\\n55.1\\n32.7\\n53.9\\n64.9\\nDINO-5scale (Zhang et al., 2022a)\\n36\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\n51.0\\n69.0\\n55.6\\n34.1\\n53.6\\n65.6\\nSAM-DETR (Zhang et al., 2022b)\\nResNet50\\n50\\n100\\n58\\nâ€“\\nâ€“\\n39.8\\n61.8\\n41.6\\n20.5\\n43.4\\n59.6\\nSAM-DETR-DC5 (Zhang et al., 2022b)\\n50\\n210\\n58\\nâ€“\\nâ€“\\n43.3\\n64.4\\n46.2\\n25.1\\n46.9\\n61.0\\nPix2Seq (Chen et al., 2021)\\nResNet50\\n50\\nâ€“\\n37\\nâ€“\\nâ€“\\n43.0\\n61.0\\n45.6\\n25.1\\n46.9\\n59.4\\nPix2Seq-DC5 (Chen et al., 2021)\\n50\\nâ€“\\n38\\nâ€“\\nâ€“\\n43.2\\n61.0\\n46.1\\n26.6\\n47.0\\n58.6\\nAdditionally, we have presented the attention visualization of the\\nencoder and decoder (as shown in Figs. 5 and 6). This visualization\\naids in understanding how the model focuses on various parts of the\\ninput image and utilizes attention mechanisms for object detection.\\nThe encoder processes the input image, captures its spatial information,\\nand creates a set of contextualized feature representations. Attention\\nvisualization in the encoder demonstrates how the model concentrates\\non specific regions of the image, emphasizing crucial areas that con-\\ntribute to the comprehension of the objects present. The decoder uses\\nthe encoded features to generate final object detections, employing a\\nseries of self-attention and cross-attention mechanisms to iteratively\\nrefine the predicted object bounding boxes and class labels.\\nIn summary, DETR, the first Transformer-based end-to-end object\\ndetector, exhibited performance comparable to state-of-the-art (SOTA)\\nmethods at the time. However, there are evident drawbacks in its\\napplication: slow convergence and low accuracy on small objects.\\nNonetheless, its end-to-end architecture possesses significant potential\\nand has attracted numerous researchers to explore improvements.\\n3.2.2. UP- DETR\\nSince DERT faces great challenges in training and optimization, it\\nrequires a huge amount of training data and an extremely long training\\nschedule, which leads to limitations in application on small datasets.\\nMoreover, the existing pretext task cannot be directly applied to train\\nthe Transformer module of DETR, because DETR focuses mainly on\\nspatial localization rather than image instance-based or cluster-based\\nsegmentation learning. To address the above issues, Dai et al. (2021a)\\nproposed UP-DETR, a DETR-like model capable of unsupervised pre-\\ntraining, whose structure is shown in Fig. 7.\\nMultiple query patches are randomly cropped from a given image\\nand the Transformer for detection is pre-trained to predict the bounding\\nboxes of these query patches in the given image. In the pre-training\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n7\\nY. Li et al.\\nFig. 5. Encoder self-attention for a set of reference points. It demonstrates the attention distribution after the input image is processed through the Transformer encoder.\\nFig. 6. Visualization of decoder attention for each predicted object in images from the COCO validation set, using the DETR-DC5 model. Attention scores are represented by\\ndistinct colors for different objects. The decoder primarily focuses on object extremities, such as legs and heads, highlighting the modelâ€™s ability to capture fine-grained details. It\\nis recommended to view this figure in color for better understanding.\\nFig. 7. UP-DETR pre-training architecture by random query patch detection: (a) For one single-query patch, which is added to all object queries. (b) For the multi-query patch,\\nwhich is added each query patch to ğ‘âˆ•ğ‘€object queries with shuffle and attention mask.\\nSource: Image form Dai et al. (2021a).\\nprocess, the method addresses the following two key problems. (1) To\\ntrade-off the preference of classification and localization in the pre-text\\ntask, the backbone network is frozen and a patch feature reconstruction\\nbranch is proposed that is jointly optimized with patch detection. (2)\\nFor multi-query patch, UP-DETR is introduced in single-query patch\\nand extended to multi-query patches with object query shuffle and\\nattention mask.\\nIn summary, UP-DETR proposes a new unsupervised pre-text task-\\nrandom query patch detection to pre-train the Transformer. The results\\nshow that UP-DETR has significantly better performance than DETR in\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n8\\nY. Li et al.\\nobject detection, panorama segmentation, and single detection, even on\\nthe PASCAL VOC dataset where the training data is insufficient.\\n3.2.3. YOLOS\\nInspired by the pre-trained Transformer can fine-tune at the token\\nlevel tasks (Rajpurkar et al., 2016; Sang and De Meulder, 2003), Fang\\net al. (2021) proposed YOLOS, a pure sequence-to-sequence trans-\\nformer on the basis of DETR (Carion et al., 2020) and ViT (Dosovitskiy\\net al., 2021). It replaces the class token of the original ViT with the\\ndetection token and replaces the image classification loss with the\\nbipartite matching loss of DETR in the training phase, which allows\\nobject detection by set prediction. YOLOS demonstrates the generality\\nand transferability of the pre-trained Transformer from image classi-\\nfication to downstream object detection task, which is pre-trained in\\nthe classification task and then transfer to the detection task for fine-\\ntuning. Experiments demonstrate that YOLOS-Base, pre-trained on only\\nmedium-sized ImageNet datasets can achieve 42.0 box AP.\\n3.2.4. Deformable DETR\\nInspired by Deformable Convolution ((Dai et al., 2017), Zhu et al.\\n(2021) proposed Deformable DETR. This method combines the ad-\\nvantages of sparse spatial sampling of deformable convolution with\\nthe relational modeling capability of Transformer. The Deformable\\nAttention Module (DEM) is introduced to accelerate convergence and\\nfuse multi-scale features to improve accuracy. Moreover, The authors\\nintroduce multi-scale feature from FPN (Lin et al., 2016), and then\\npropose Multi-Scale Deformable Attention (MSDA) to replace the Trans-\\nformer Attention Module for processing feature maps, as shown in\\nFig. 8 is shown. Let {ğ’™ğ‘™}ğ¿\\nğ‘™=1 be the input multi-scale feature map, where\\nğ‘¥ğ‘™âˆˆRğ¶Ã—ğ»ğ‘™Ã—ğ‘Šğ‘™. Let Ì‚ğ’‘ğ‘âˆˆ[0, 1]2 be the normalized coordinates of the\\nreference point of each query element ğ‘, and then compute Multi-Scale\\nDeformable Attention as\\nDeformAttn (ğ’›ğ‘, ğ’‘ğ‘, ğ’™) =\\nğ‘€\\nâˆ‘\\nğ‘š=1\\nğ‘¾ğ‘š\\n[ ğ¾\\nâˆ‘\\nğ‘˜=1\\nğ´ğ‘šğ‘ğ‘˜â‹…ğ‘¾â€²\\nğ‘šğ’™(ğ’‘ğ‘+ ğ›¥ğ’‘ğ‘šğ‘ğ‘˜\\n)\\n]\\n,\\n(14)\\nwhere ğ‘šis the index of attention head, ğ‘™is the index of input feature\\nlevel, and k is the index of sampling points. ğ›¥ğ’‘ğ‘šğ‘™ğ‘ğ‘˜and ğ´ğ‘šğ‘™ğ‘ğ‘˜denote\\nrespectively sampling offset and attention weight of the ğ‘˜th sampling\\npoint in the ğ‘™th feature layer and the ğ‘šth attention head. The scalar\\nattention weights ğ´ğ‘šğ‘™ğ‘ğ‘˜are normalized to âˆ‘ğ¿\\nğ‘™=1\\nâˆ‘ğ¾\\nğ‘˜=1 ğ´ğ‘šğ‘™ğ‘ğ‘˜= 1. The\\nnormalized coordinates (0, 0) and (1, 1) of Ì‚ğ’‘ğ‘âˆˆ[0, 1]2 denote the upper\\nleft and lower right corners of the image, respectively. The function\\nğœ™ğ‘™\\n(Ì‚ğ’‘ğ‘\\n) in Eq. (14) rescales the normalized coordinates ğ‘·ğ‘to the input\\nfeature map of the ğ‘™th layer. The computational complexity of MSDA\\nis ğ‘‚(2ğ‘ğ‘ğ¶2 + min (ğ»ğ‘Šğ¶2, ğ‘ğ‘ğ¾ğ¶2)) compared to the original DETR,\\nDeformable DETR requires less than one-tenth of training epochs to\\nachieve better performance (especially on small object).\\n3.2.5. Conditional DETR\\nMeng et al. (2021) proposed Conditional DETR. They visualized ex-\\nperiments on the operation of DETR and concluded that cross-attention\\nin DETR is highly dependent on content embedding to locate the\\nfour vertices and predict the bounding box. Thus, it increases the\\ntraining difficulty. So they improved the cross-attention of DETR by\\nconcatenating the content query ğ‘ğ‘and spatial query ğ‘ğ‘, and the key\\nby splicing the content key ğ‘ğ‘˜and spatial key ğ‘ğ‘˜. This inner product of\\nquery and key gives the following result:\\nğœâŠ¤\\nğ‘ğœğ‘˜+ ğ©âŠ¤\\nğ‘ğ©ğ‘˜,\\n(15)\\nThis separates the functions of content query and spatial query\\nso that they focus on the weight of content and space respectively.\\nAs shown in Fig. 9, the improved Decoder layer consists of three\\nmain modules: 1) The self-attention layer, which is from the previous\\nDecoder layer and is used to remove duplicate predictions as well as\\npredict categories and bounding boxes; (2) The cross-attention layer,\\nwhich can use embedding output of encoder to complete the embedding\\nof the decoder; (3) The feed-forward networks layer (FFN).\\nThe core of the conditional cross-attention mechanism is to learn\\na conditional spatial query from decoder embedding and reference\\npoints, which can explicitly find the boundary regions of the object,\\nthus narrowing down the search object, helping to locate the object,\\nand alleviating the problem of over-reliance on the quality of content\\nembedding in DETR training. The problem of over-reliance on the\\nquality of content embedding in DETR training is alleviated. These\\nrefinements improve the convergence speed of DETR by 8Ã— faster and\\nthe box mAP on the COCO dataset by 1.8%.\\n3.2.6. Efficient DETR\\nYao et al. analyzed the mechanisms of DETR and Deformable DETR\\nand found that their common feature is a cascade structure stacked\\nwith six Decoders, which is used to iteratively update the object query.\\nThe reference point proposed by Deformable DETR visualizes the object\\nquery and solves the difficult problem that the object query is difficult\\nto analyze directly. However, different initialization methods of refer-\\nence points have a great impact on decoder performance. In order to\\ninvestigate a more efficient way to initialize the object container, Yao\\net al. proposed Efficient DETR, a two-stage object detector that consists\\nof dense prediction and sparse set prediction, and these two parts share\\nthe same detection head.\\nThe model generates region proposals using dense detection before\\ninitializing the object container, and then uses the highest-scoring 4-\\ndimensional proposal and its 256-dimensional encoder features as the\\ninitialization value of the object container, which results in better\\nperformance and fast convergence. The experimental results show that\\nEfficient DETR combines the features of dense detection and ensemble\\ndetection, and can converge quickly while achieving high performance.\\nThe model achieves the SOTA performance at that time on the COCO\\ndataset with only one encoder layer and three decoder layers, while the\\nepoch is reduced by 14Ã— less.\\n3.2.7. SMCA\\nTo strengthen the relationship between the visual region of common\\ninterest for each object query and the bounding box to be predicted by\\nthe query, Gao et al. (2021) introduced spatial prior and multi-scale fea-\\ntures, and proposed Spatially Modulated Co Attention (SMCA), which\\nreplaces the cross attention in the original Decoder while keeping the\\nothers unchanged.\\nThe decoder of SMCA has multiple cross-attention heads, each of\\nwhich estimates the object center and scale from a slightly different\\nlocation, resulting in a series of different spatial weight maps. This\\nweight map is used to spatially adjust the co-attention features, which\\nimproves the detection performance. Based on these improvements,\\nSMCA can achieve 43.7 mAP in 50 epochs and 45.6 mAP in 108 epochs\\non the COCO dataset.\\n3.2.8. ACT\\nDue to the slow convergence of DETR, Zheng et al. (2021) proposed\\nthe Adaptive Clustering Transformer (ACT) to address the problem of\\nhigh trial and error costs for improving DETR. ACT is a plug-and-\\nplay module that is fully compatible with Transformer and can be\\nported to DETR without any training. Its core design is first, to perform\\nfeature clustering adaptively for the attention redundancy (points with\\nsimilar semantics and similar spatial locations produce similar atten-\\ntion maps) of encoder, select representative prototypes, and broadcast\\nfeature updates to their nearest neighboring points based on Euclidean\\ndistance. Second, an adaptive clustering algorithm is designed for the\\nencoder note feature diversity problem (for different inputs, the feature\\ndistribution of each encoder layer is quite different), and a multi-\\nround exact Euclidean location-sensitive hash (E2LSH) is chosen for this\\nalgorithm to adaptively determine the number of prototypes. Thanks\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n9\\nY. Li et al.\\nFig. 8. The architecture of Deformable DETR. Its attention module focuses on only a small number of key sampling points around the reference point, and assigns a fixed and\\nsmall number of keys to each object query, thus alleviating the problems of slow convergence and low feature resolution.\\nSource: Image from Zhu et al. (2021).\\nFig. 9. A Decoder layer of Conditional DETR. The gray shaded box indicates that the\\nConditional spatial query is predicted from the learnable 2D coordinates ğ‘ and the\\nembedding output of the previous Decoder layer.\\nSource: Image from Meng et al. (2021).\\nto these improvements, ACT can reduce the FLOPS of DETR from\\n73.4 Gflops to 58.2 Gflops (excluding Backbone Resnet FLOPs) without\\nadditional training, while the loss of AP is only 0.7%. The AP loss\\ncan be further reduced to 0.2% by multitasking knowledge distillation.\\nGiven its excellent performance, exploring ACT training from scratch\\nand fusion with multi-scale features is a worthy research direction in\\nthe future.\\n3.2.9. TSP\\nSun et al. (2021) concluded after a lot of analysis that the cross-\\nattention part of decoder and Hungarian loss of DETR are the main\\nreasons for the slow convergence of DETR. So they proposed two\\nimproved models of DETR with only encoder, TSP-FCOS and TSP-\\nRCNN corresponding to the One-Stage and Two-Stage object detection\\nmethods, respectively. Both models can be viewed as feature pyra-\\nmid (Lin et al., 2016) based. The model uses a feature of interest (FoI)\\nselection mechanism that helps encoder process multi-scale features. In\\naddition, the model applies matching distillation to solve the instability\\nof bipartite graph matching. Experiments show that TSP achieves better\\nresults with reduced training cost, using only 36-epoch to achieve the\\n500-epoch results of the original DETR training.\\n3.2.10. DINO\\nThe Hungarian algorithm has been used in DETR (Carion et al.,\\n2020) to match the output of the object by Decoder with Ground Truth.\\nHowever, the discreteness of the Hungarian algorithm matching and\\nthe randomness of the model training cause the matching process to be\\ndynamic and unstable, resulting the final slow convergence of DETR.\\nBy deeply studying the iteration mechanism and optimization prob-\\nlems of the DETR model, Zhang et al. (2022a)proposed DINO (DETR\\nwith Improved deNoising anchor boxes) based on DN-DETR (Li et al.,\\n2022), DAB-DETR (Liu et al., 2022a) and Deformable DETR (Zhu et al.,\\n2021). The key design of DINO is that the training phase uses denoising\\ntraining as a shortcut to learning the relative offset of anchor by first\\nadding noise near the Ground Truth box, and then the Hungarian\\nmatching directly reconstructs the truth bounding box, thus improving\\nthe stability of matching. Secondly, the model also uses a query-based\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n10\\nY. Li et al.\\nFig. 10. The architecture of Pyramid Vision Transformer, the whole model is divided into 4 stages to generate feature maps at different scales. Each stage consists of a patch\\nembedding layer, ğ¿ğ‘–-layer, and reshape operation.\\nSource: Image form Wang et al. (2021a).\\ndynamic anchor formulation to initialize the query and correct the\\nparameters of adjacent earlier layers with the gradients of later layers.\\nDINO breaks the dominance of classical architecture detector (SwinV2-\\nG (Liu et al., 2021a), Florence (Yuan et al., 2021), DyHead (Dai\\net al., 2021b), etc.). DINO-Res50, which combines multi-scale features,\\nachieves 48.3AP and 51.0AP on the COCO2017 dataset with 12-epoch\\nand 36-epoch training schemes, respectively. Moreover, DINO-Swin-L\\neven achieves the highest performance 63.3AP after training on a larger\\ndataset.\\n3.3. Transformer backbone\\nOther efforts such as ViT (Dosovitskiy et al., 2021) have used Trans-\\nformer in the image classification and achieved comparable results.\\nHowever, there are some limitations in other complex CV tasks. These\\nchallenges of transferring the high performance of Transformer in NLP\\nto the CV can be explained by the differences between the two domains.\\n1. The object entities in CV tasks often have dramatic scale varia-\\ntion.\\n2. Compared to text, the matrix nature of images makes it con-\\ntain at least hundreds of pixels for an image that can express\\ninformation. Especially the very long sequence unfolded by high-\\nresolution images is difficult for Transformer to model.\\n3. Many CV tasks such as semantic segmentation require pixel-level\\ndense prediction, and the computational complexity of the self-\\nattention mechanism in ViT increases quadratically with image\\nsize, which leads to unacceptable computational overhead.\\n4. In the existing Transformer-based models, tokens are fixed in\\nscale and not improved in design for CV tasks.\\nTo address the above challenges, many Transformer-based back-\\nbones have been proposed for CV tasks and combined with methods\\nsuch as multi-scale to compensate for the shortcomings that ViT can\\nonly detect at low resolution and so on. These methods can replace the\\nbackbone of mainstream object detection models, and in the benchmark\\nTable 5, we list the performance of Mask R-CNN (He et al., 2017)\\nand RetinaNet (Ross and DollÃ¡r, 2017) comparison after replacing the\\nbackbone and review the classical models in this subsection.\\n3.3.1. PVT&PVTv2\\nThe feature maps output by ViT (Dosovitskiy et al., 2021) are\\ndifficult to apply to dense prediction due to their single scale and low\\nresolution. Wang et al. (2021a) proposed the Pyramid Vision Trans-\\nformer (PVT) by incorporating the multi-scale feature into Transformer.\\nPVT can be used as a Backbone for various dense detection tasks,\\nespecially it can replace the CNN backbone of DETR-like models or be\\ncombined into a pure Transformer model without manual components\\nsuch as NMS.\\nBenefiting from the progressive shrinking pyramid structure in the\\nPVT, the Transformer sequence length decreases as the network gets\\ndeeper. Meanwhile, in order to further reduce the computation of\\nfine-grained segmentation of images, they propose spatial-reduction\\nattention (SRA) to reduce the computation of learning high-resolution\\nfeature maps (As shown in Fig. 10).\\nCompared with the CNN method based on feature pyramid struc-\\nture, PVT not only generates multi-scale feature maps to detect ob-\\njects of different sizes but also fuses global information through self-\\nattention mechanism. The PVTv2 (Wang et al., 2022) proposed by\\nthe same team subsequently improves the PVT by adding a linear\\ncomplexity attention layer, overlapping patch embedding, and convo-\\nlutional feed-forward network to improve the performance of the PVT\\nas backbone. On the COCO dataset, both achieved competitive results\\nat that time.\\n3.3.2. Swin transformer\\nLiu et al. (2021b) proposed Swin Transformer, which creatively uses\\na hierarchical design to make the Transformer available as a backbone\\nfor most CV tasks, rather than just a detection head. As shown in\\nFig. 11, It is easy to see that, unlike other Transformer models, Swin\\nTransformer builds a feature map with hierarchical representation,\\nsimilar to the feature pyramid structure in CNN. As the network level\\ndeepens, the receptive expands, enabling the extraction of multi-scale\\nfeatures of the image. Secondly, Swin Transformer divides the feature\\nmap with multiple windows, and each non-overlapping window per-\\nforms local multi-head attention calculation without correspondence\\nbetween windows, which makes the computation greatly reduced and\\nlinear with the image size, as shown in the Eq. (16). In contrast, ViT\\nproduces a single low-resolution image and calculates global attention,\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n11\\nY. Li et al.\\nFig. 11. Compare Swin Transformer (left) with ViT (right).\\nSource: Image form Liu et al. (2021b).\\nso the computational complexity and image size are quadratically\\nrelated, as shown in Eq. (17)\\nğ›º(W âˆ’MSA) = 4â„ğ‘¤ğ¶2 + 2ğ‘€2â„ğ‘¤ğ¶,\\n(16)\\nğ›º(MSA) = 4â„ğ‘¤ğ¶2 + 2(â„ğ‘¤)2ğ¶,\\n(17)\\nwhere M is a fixed window size (set to 7 by default), computing global\\nattention for ViT is unacceptable for large image sizes ğ»ğ‘Š, while\\nwindow-based multi-head self-attention (W-MSA) is scalable.\\nThe Pipeline of the Swin Transformer is shown in Fig. 12(a). The\\ninput image is spreading into a sequence after Patch Partition and\\nLinear Embedding layers, and then input into 4 stages. The Swin\\nTransformer block in each stage replaces the standard multi-head self-\\nattention (MSA) module in the Transformer module with window-based\\nself-attention (W-MSA) or a shift window-based module (SW-MSA),\\nwhich introduces a relative position bias in the computation of at-\\ntention to account for the geometric relationships in the self-attention\\ncomputation, as shown in Eq. (18). This parameter accounts for the\\nrelative spatial configuration of the visual elements and is shown to be\\ncritical in various visual tasks, especially for intensive recognition tasks\\nsuch as object detection and semantic segmentation.\\nAttention(ğ‘„, ğ¾, ğ‘‰) = SoftMax\\n(\\nğ‘„ğ¾ğ‘‡âˆ•\\nâˆš\\nğ‘‘+ ğµ\\n)\\nğ‘‰,\\n(18)\\nAlthough W-MSA reduces the computation greatly, W-MSA loses the\\nability to model the relationship between different windows, and the\\nlack of information exchange between non-overlapping windows affects\\nthe representation of the model. So they introduced SW-MSA, which\\nshifts the windows in the Swin Transformer Block of the next layer to\\nintroduce correspondence of the previous layer. This operation greatly\\nincreases the actual receptive field, as shown in Fig. 13. In this way,\\nthe multi-head attention is computed inside the new window to include\\nthe boundary of the original window and achieve the modeling of the\\nrelationship between windows.\\nBut this approach causes the number of windows to change, and\\nthe window size is not uniform. An easy way to solve this problem\\nis padding the small window, but it will increase the computation. So\\nthey proposed cyclic-shifting, a more efficient method of batch com-\\nputation. This method cyclically shifts and merges small windows, so\\nthat a window may contain content from different windows, and there-\\nfore the masked MSA mechanism is used to restrict the self-attention\\ncomputation to each sub-window, as shown in Fig. 14.\\nSwin Transformer has achieved SOTA performance on classification,\\ndetection, and segmentation tasks. Its biggest contribution is to propose\\na backbone that can be widely used in CV. And most of the hyper-\\nparameters commonly found in CNNs can be manually tuned in Swin\\nTransformer, such as the number of network blocks and the size of input\\nimages. This method combines the advantages of both Transformer and\\nCNN, fully considers the size invariance of CNN and the relationship\\nbetween receptive field and number of layers, and solves the problem\\nof slow application of Transformer in CV.\\n3.3.3. Swin TransformerV2\\nAfter Swin Transformer, Liu et al. (2021a) proposed Swin Trans-\\nformerV2 to address the problems of expansion of CV models and\\ntraining with high-resolution images, as well as the excessive GPU\\nmemory consumption for large models. Swin Transformer is optimized\\nto scale up to 3 billion parameters and can be trained with images up\\nto 1536 Ã— 1536 resolutions. The improved method is shown in Fig. 15.\\nPost normalization technique: They found that when scaling up\\nthe model, the activation values in the deep layer increase dramatically.\\nIn fact, in the pre-normalized configuration (Layer Norm layer before\\nthe Attention layer), the output activation values of each residual block\\nare directly merged back to the main branch, and the amplitude of the\\nmain branch becomes in the deeper layers. The huge amplitude differ-\\nences between different layers may cause training instability problems.\\nTherefore, they propose a post normalization technique, in which the\\noutput of each residual block is normalized before it is merged back\\ninto the main branch, and the amplitude of the main branch does not\\naccumulate as the number of layers deepens.\\nScaled cosine attention: In the original self-attention computation,\\nthe similarity terms of pixel pairs are computed as dot products of\\nqueries and keys vectors. However, when using this approach for\\nlarge visual models, the learned attention graph for some blocks and\\nattention heads is often dominated by several pixel pairs, especially\\nin post-normalization configurations. To alleviate this problem, the\\nauthors propose a scaled cosine attention (Scaled cosine attention)\\nmethod, which computes the number of attention pairs for a pixel pair\\nğ‘–and ğ‘—by a scaled cosine function:\\nSim (ğªğ‘–, ğ¤ğ‘—\\n) = cos (ğªğ‘–, ğ¤ğ‘—\\n) âˆ•ğœ+ ğµğ‘–ğ‘—,\\n(19)\\nwhere ğµğ‘–ğ‘—is the relative position bias between pixels ğ‘–and ğ‘—; ğœis a\\nlearnable scalar that cannot be shared across heads and layers. The ğœis\\nset to be greater than 0.01. The cosine function is naturally normalized\\nso that it can have milder attention values, which improves the stability\\nof large visual models and makes the model capacity easier to be scaled\\nup.\\nLog-spaced continuous position bias:They found that the original\\nrelative position encoding method was weak for scale generalization of\\nthe model, and proposed log-spaced continuous position bias so that\\nthe relative position bias can be transferred smoothly across windows at\\ndifferent resolutions, effectively transferring models pre-trained in low-\\nresolution images and windows to their higher resolution counterparts:\\nÌ‚\\nğ›¥ğ‘¥= sign(ğ‘¥) â‹…log(1 + |ğ›¥ğ‘¥|),\\nÌ‚\\nğ›¥ğ‘¦= sign(ğ‘¦) â‹…log(1 + |ğ›¥ğ‘¦|),\\n(20)\\nwhere ğ›¿ğ‘¥, ğ›¿ğ‘¦and Ì‚\\nğ›¥ğ‘¥, Ì‚\\nğ›¥ğ‘¦are the coordinates of linear scale and\\nlogarithmic space, respectively. The optimized resulting architecture\\nwas named Swin TransformerV2, and the model achieved a box/mask\\nmAP of 63.1/54 in the COCO2017 dataset.\\n3.3.4. Other representative methods\\nIn addition to the conventional approaches, our benchmark extends\\nto include comparisons with several cutting-edge techniques. ViL, in-\\ntroduced by Zhang et al. (2021), realizes a multi-scale configuration\\nthrough the sequential stacking of numerous ViT stages. Furthermore,\\nit enhances the attention mechanism, thus elevating both efficiency and\\nclassification performance. The Focal Transformer introduces the novel\\nFocal Self-Attention mechanism. This integrates both granular local and\\ncoarse global interactions, thereby ensuring the effective capturing of\\nboth proximal and distal visual dependencies. Twins (Chu et al., 2021)\\npropose two highly efficient vision transformer architectures, Twins-\\nPCPVT and Twins-SVT, both leveraging a restructured spatial attention\\nmechanism. This methodology incorporates both locally-grouped self-\\nattention and global sub-sampled attention, capturing both fine-grained\\nproximal and coarse-grained distal visual information. Dong et al.\\n(2022) introduced the CSWin Transformer, a robust Transformer-based\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n12\\nY. Li et al.\\nFig. 12. (a) Swin Transformer (Swin-T) (b) Swin Transformer Block.\\nSource: Image from Liu et al. (2021b).\\nFig. 13. The shift window approach can calculates the self-attention across the window\\nboundary of the previous layer.\\nSource: Image from Liu et al. (2021b).\\nFig. 14. An illustration of circular shift.\\nSource: Image from Liu et al. (2021b).\\nFig. 15. Comparison of the attention modules of Swin TransformerV1 and V2.\\nSource: Image from Liu et al. (2021a).\\nbackbone for vision tasks. It integrates the Cross-Shaped Window self-\\nattention mechanism, varies stripe widths based on network depth,\\nand introduces a novel Locally-enhanced Positional Encoding (LePE)\\nscheme to handle local positional information optimally, resulting in\\ncompetitive performance across standard vision tasks.\\n3.4. Analysis and discussion for detectors\\nThis section provides a succinct review of conventional Transformer-\\nbased object detectors, offering a detailed performance comparison in\\nTables 4 and 5. Each method was evaluated using the NVIDIA A100\\nGPU and adhered to the DETR training protocol. The AdamW opti-\\nmizer (Loshchilov and Hutter, 2017) was uniformly employed across all\\nmethods, with the initial learning rate for the transformer set to 10âˆ’4,\\nthe backboneâ€™s to 10âˆ’5, and weight decay at 10âˆ’4. The transformer\\nweights were initialized with Xavier init (Glorot and Bengio, 2010),\\nwhile the backbone leveraged the ImageNet-pretrained ResNet model\\nfrom torchvision, with frozen batch normalization layers.\\nFor Transformer Neck-based models, they treat object detection as\\na straightforward set prediction, removing manual components (such\\nas anchor set and NMS) that cannot be optimized, thus enabling end-\\nto-end detection. Starting from the original DETR with slow conver-\\ngence and poor detection of small objects, subsequent researchers have\\nproposed optimization strategies from different perspectives.\\n1. To address the problem of slow convergence, researchers often\\nstart by improving the attention mechanism. Deformable DETR (Zhu\\net al., 2021) accelerates convergence 12Ã— faster with the Deformable\\nAttention Module. Conditional DETR improves the cross-attention of\\nDETR and gets 8Ã— faster convergence. Meanwhile, the box mAP on\\nthe COCO dataset is improved by 1.8%. Unlike the above methods,\\nACT (Zheng et al., 2021) proposes a plug-and-play module for adaptive\\nclustering, which reduces the GFLOPS of DETR by 15.2 without addi-\\ntional training, while the AP loss is only 0.7%. Sparse DETR achieves\\nhigher performance and the same detection speed (FPS) as Faster\\nR-CNN by improving and reducing the GFLOPs by 75.\\n2. For the problem of poor detection of small objects, multi-scale\\nfeature is currently the main focus. Methods such as SMCA (Gao et al.,\\n2021) (as shown in Table 4) introduce multi-scale feature with different\\noperations and significantly improve the accuracy of the detector.\\nMoreover, DINO (Zhang et al., 2022a) reaches 63.3 AP over all classical\\nobject detection methods.\\nPresently, most Transformer Backbones are primarily active in im-\\nage classification, with only a few researchers transitioning them to tra-\\nditional object detectors for dense prediction. These have then achieved\\nstate-of-the-art (SOTA) performance. Compared to CNN-based Back-\\nbones, Transformer-based Backbones can integrate global contextual\\ninformation while outputting multi-scale feature maps, thereby enhanc-\\ning feature extraction. Although Transformers have challenged CNNâ€™s\\ndominance in object detection, recent advancements such as FAIRâ€™s\\nredesign of ConvNet (Liu et al., 2022b), which draws from the strengths\\nof the Transformer structure, underscore the continued potential of\\nCNNs. In the future, CNN and visual Transformer are expected to\\ncontinue improving by leveraging each otherâ€™s strengths.\\n4. Discussion\\nAlthough the Transformer model has made great progress (as shown\\nin Table 6) and has shown excellent performance (Table 4, Table 5),\\nthey still face some challenges, as well as limitations in practical\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n13\\nY. Li et al.\\nTable 5\\nThe prediction results of RetinaNet and Mask R-CNN with Transformer as Backbone on COCO 2017 Val Set. Where 3 Ã— schedule denotes 36-epoch, MS denotes multi-scale input\\n(MS), and the numbers before and after â€˜â€˜/â€™â€™ denote the parameters of RetinaNet and Mask R-CNN, respectively.\\nBackbone\\n#Params\\nFlOPs\\nRetinaNet 3Ã—schedule + MS\\nMask R-CNN 3Ã—schedule + MS\\n(M)\\n(G)\\nğ€ğğ‘\\nğ€ğğ‘\\n50\\nğ€ğğ‘\\n75\\nğ€ğğ‘†\\nğ€ğğ‘€\\nğ€ğğ¿\\nğ€ğğ‘\\nğ€ğğ‘\\n50\\nğ€ğğ‘\\n75\\nğ€ğğ‘š\\nğ€ğğ‘š\\n50\\nğ€ğğ‘š\\n75\\nResNet50 (He et al., 2015)\\n38/44\\n239/260\\n39\\n58.4\\n41.8\\n22.4\\n42.8\\n51.6\\n41\\n61.7\\n44.9\\n37.1\\n58.4\\n40.1\\nPVTv1-S (Wang et al., 2021a)\\n34/44\\n226/245\\n42.2\\n62.7\\n45.0\\n26.2\\n45.2\\n57.2\\n43.0\\n65.3\\n46.9\\n39.9\\n62.5\\n42.8\\nViL-S (Zhang et al., 2021)\\n36/45\\n252/174\\n42.9\\n63.8\\n45.6\\n27.8\\n46.4\\n56.3\\n43.4\\n64.9\\n47.0\\n39.6\\n62.1\\n42.4\\nSwin-T (Liu et al., 2021b)\\n39/48\\n245/264\\n45.0\\n65.9\\n48.4\\n29.7\\n48.9\\n58.1\\n46.0\\n68.1\\n50.3\\n41.6\\n65.1\\n44.9\\nPVTv2-B2-Li (Liu et al., 2021b)\\n32/42\\n-/-\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\n46.8\\n68.7\\n51..4\\n42.3\\n65.7\\n45.4\\nFocal-T (Yang et al., 2021)\\n39/49\\n265/291\\n45.5\\n66.3\\n48.8\\n31.2\\n49.2\\n58.7\\n47.2\\n69.4\\n51.9\\n42.7\\n66.5\\n45.9\\nTwinsP-S (Chu et al., 2021)\\n34/44\\n-/245\\n45.2\\n66.5\\n48.6\\n30.0\\n48.8\\n58.9\\n46.8\\n69.3\\n51.8\\n42.6\\n66.3\\n46.0\\nTwins-S (Chu et al., 2021)\\n34/55\\n-/228\\n45.6\\n67.1\\n48.6\\n29.8\\n49.3\\n60.0\\n46.8\\n69.2\\n51.2\\n42.6\\n66.3\\n45.8\\nCSwin-T (Dong et al., 2022)\\n-/42\\n-/279\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\n49.0\\n70.7\\n53.7\\n43.6\\n67.9\\n46.6\\nPVTv2-B2 (Wang et al., 2022)\\n35/45\\n-/-\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\n47.8\\n69.7\\n52.6\\n43.1\\n66.8\\n46.7\\nResNet101 (He et al., 2015)\\n57/63\\n315/336\\n40.9\\n60.1\\n44.0\\n23.7\\n45.0\\n53.8\\n42.8\\n63.2\\n47.1\\n38.5\\n60.1\\n41.3\\nResNeXt101-32 Ã— 4d (He et al., 2015)\\n56/63\\n319/340\\n41.4\\n61 .0\\n44.3\\n23.9\\n45.5\\n53.7\\n44.0\\n64.4\\n48.0\\n39.2\\n61.4\\n41.9\\nPVTv1-M (Wang et al., 2021a)\\n54/64\\n283/302\\n43.2\\n63.8\\n46.1\\n27.3\\n46.3\\n58.9\\n44.2\\n66.0\\n48.2\\n40.5\\n63.1\\n43.5\\nViL-M (Zhang et al., 2021)\\n51/60\\n339/261\\n43.7\\n64.6\\n46.4\\n27.9\\n47.1\\n56.9\\n44.6\\n66.3\\n48.5\\n40.7\\n63.8\\n43.7\\nTwinsP-B (Chu et al., 2021)\\n54/64\\n-/302\\n46.4\\n67.7\\n49.8\\n31.3\\n50.2\\n61.4\\n47.9\\n70.1\\n52.5\\n43.2\\n67.2\\n46.3\\nTwins-B (Chu et al., 2021)\\n67/76\\n-/340\\n46.9\\n68.0\\n50.2\\n31.7\\n50.3\\n61.8\\n48.0\\n69.5\\n52.7\\n43.0\\n66.8\\n46.6\\nSwin-Scite (Liu et al., 2021b)\\n60/69\\n335/354\\n46.4\\n67.0\\n50.1\\n31.0\\n50.1\\n60.3\\n48.5\\n70.2\\n53.5\\n43.3\\n67.3\\n46.6\\nFocal-S (Yang et al., 2021)\\n62/71\\n367/401\\n47.3\\n67.8\\n51.0\\n31.6\\n50.9\\n61.1\\n48.8\\n70.5\\n53.6\\n43.8\\n67.7\\n47.2\\nCSwin-S (Dong et al., 2022)\\n-/54\\n-/342\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\n50.0\\n71.3\\n54.7\\n44.5\\n68.4\\n47.7\\nResNeXt101-64 Ã— 4d (He et al., 2015)\\n96/102\\n473/493\\n41.8\\n61.5\\n44.4\\n25.2\\n45.4\\n54.6\\n44.4\\n64.9\\n48.8\\n39.7\\n61.9\\n42.6\\nPVTv1-Large (Wang et al., 2021a)\\n71/81\\n345/364\\n43.4\\n63.6\\n46.1\\n26.1\\n46.0\\n59.5\\n44.5\\n66.0\\n48.3\\n40.7\\n63.4\\n43.7\\nViL-Base (Zhang et al., 2021)\\n67/76\\n443/365\\n44.7\\n65.5\\n47.6\\n29.9\\n48.0\\n58.1\\n45.7\\n67.2\\n49.9\\n41.3\\n64.4\\n44.5\\nSwin-Base (Liu et al., 2021b)\\n98/107\\n477/496\\n45.8\\n66.4\\n49.1\\n29.9\\n49.4\\n60.3\\n48.5\\n69.8\\n53.2\\n43.4\\n66.8\\n46.9\\nFocal-Base (Yang et al., 2021)\\n101/110\\n514/533\\n46.9\\n67.8\\n50.3\\n31.9\\n50.3\\n61.5\\n49.0\\n70.1\\n53.6\\n43.7\\n67.6\\n47.0\\nCSWin-B (Dong et al., 2022)\\n-/97\\n-/526\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\nâ€“\\n50.8\\n72.1\\n55.8\\n44.9\\n69.1\\n48.3\\napplications. This section will summarize the innovative improvements\\nof the current method, analyze the problems encountered by the\\nTransformer detector, and give an outlook on the future development\\nprospects.\\n4.1. Challenges\\nHigh computational overhead. Typical properties of CNNs include\\ninductive bias, which is expressed as translation invariance, weight\\nsharing, and sparse connectivity (Dosovitskiy et al., 2021). These prop-\\nerties grant CNNs a robust local feature extraction capability and enable\\nthem to achieve high performance through the simple sliding match-\\ning of convolutional kernels. As a result, compared to Transformers,\\nCNNs often exhibit competitive performance with lower computational\\noverhead. However, current CNN architectures possess less potential\\nthan Transformers due to their weaker extraction of global features and\\ncontextual information. The self-attention mechanism in Transformers\\ncan also emulate convolutional layers, requiring only a sufficient num-\\nber of heads to focus on each pixel within the convolutional receptive\\nfield and employing relative positional encoding to ensure translation\\ninvariance (Cordonnier et al., 2020). This full-attention operation can\\neffectively integrate local and global attention while dynamically gen-\\nerating attention weights based on feature relationships. Nevertheless,\\nTransformers face certain limitations in practical applications. One of\\nthe main challenges stems from their high computational complexity.\\nThe expensive computational overhead restricts the application\\nof Transformer-based detectors on mobile computing platforms. At\\npresent, most mobile detection platforms primarily rely on one-stage\\ndetectors (Zhao et al., 2019), while the trend for Transformer de-\\ntectors leans towards offline high-precision detection. Additionally,\\nTransformers require large amounts of data, and common solutions\\ninclude data augmentation, self-supervised, or semi-supervised learning\\napproaches (He et al., 2021). Compared to state-of-the-art CNN-based\\napproaches, their deployment on mobile platforms is constrained by\\nhigher computational complexity.\\nThe impact of computational overhead on deploying Transformer-\\nbased object detection models in practical scenarios is influenced by\\nfactors such as the number of parameters, running time (FPS), and\\nfloating-point operations (FLOPs). However, these metricsâ€™ influence\\nvaries depending on the application context and hardware environ-\\nment. For example, in situations like autonomous driving or robotic\\nnavigation, FPS is a critical factor, as algorithms must process video\\nstreams at high frame rates to respond quickly to external changes.\\nIn the case of mobile devices and embedded systems, the number of\\nparameters and FLOPs are more influential due to energy and memory\\nconstraints. Consequently, deploying algorithms on mobile platforms\\nnecessitates balancing performance, energy consumption, and memory\\nusage. In cloud computing and high-performance hardware settings,\\ncomputational overhead is not the most critical factor since compu-\\ntational resources are relatively abundant. In these scenarios, model\\nperformance and accuracy are paramount.\\nAccording to the data in Table 4, Table 5, modern Transformer-\\nbased models have outperformed classical two-stage object detection\\nalgorithms (e.g., Faster R-CNN) in terms of FPS and achieved improved\\naccuracy, rendering them viable for practical applications. To ensure\\nefficient deployment and application in real-world engineering sce-\\nnarios, researchers typically optimize object detection algorithms for\\nspecific contexts, minimizing computational overhead and improving\\nreal-time performance and energy efficiency. This optimization may\\ninvolve techniques such as model compression, knowledge distillation,\\nand network architecture design.\\nInsufficient understanding of visual Transformer. Compared to\\nthe well-established research and applications of CNNs, our current un-\\nderstanding of the underlying mechanisms behind visual Transformers\\nis still limited. The Transformer architecture was originally designed for\\nsequence processing tasks (Vaswani et al., 2017). Although Transform-\\ners have demonstrated strong performance when applied to computer\\nvision tasks, there is relatively little explanation regarding their specific\\nroles and functions in this context. Consequently, gaining a deeper\\nunderstanding of the principles behind visual Transformers is crucial\\nto facilitate more fundamental optimization improvements and en-\\nhance the modelâ€™s interpretability. This deeper understanding could\\npotentially involve investigating the attention mechanisms, hierarchical\\nfeature representation, and the interaction between different layers\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n14\\nY. Li et al.\\nTable 6\\nSummary of the advantages and limitations of Transformer-based object detection models.\\nType\\nMethod\\nHighlights\\nLimitations\\nTransformer Neck\\nDETR (Carion et al., 2020)\\n(1) Proposed Transformer-based end-to-end object\\ndetection framework, (2) Removed hand-designed\\nanchor set and non-maximal suppression (NMS)\\n(1) Requires massive dataset\\ntraining, (2) Convergence is\\nvery slow, (3) Poor\\nperformance for small objects.\\nSMCA (Gao et al., 2021)\\n(1) Combining a learnable co-attention map and a\\nmanual space prior speeds up the convergence of\\nDETR, (2) Incorporating a scale selection network\\nin decoder.\\n(1) Good performance for\\nlarge objects and poor\\nperformance for small objects,\\n(2) High computational\\noverhead.\\nDeformable DETR (Zhu et al., 2021)\\n(1) Proposed deformable attention mechanism,\\nwhich pays more attention to local information\\nand improves convergence speed; (2) Combined\\nwith multi-scale feature, (3) Proposed reference\\npoint visualization object query, (4) Two-stage\\nDeformable DETR is also proposed.\\n(1) Low accuracy for large\\nobjects, (2) Deformable\\nattention brings unordered\\nmemory access, (3) High\\ncomputational overhead.\\nEfficient DETR (Yao et al., 2021)\\n(1) They found that different object container\\ninitialization methods have a great impact on\\ndecoder; (2) They also proposed an efficient way\\nof initializing object containers using the\\ncharacteristics of dense detection and sparse\\ndetection.\\n(1) Poor performance for\\nsmall objects, (2) High\\ncomputational overhead.\\nDINO (Zhang et al., 2022a)\\n(1) Propose a contrast denoising training method,\\n(2) Combine class DETR and two-stage model and\\npropose a mixed query selection method to better\\ninitialize object query, (3) Look Forward Twice:\\nIntroducing proximity layer information to update\\nparameters and improve the detection of small\\nobjects.\\n(1) High computational\\noverhead at high scales, (2)\\nDiminishing marginal benefit\\nfrom stacking too many scales.\\nYOLOS (Fang et al., 2021)\\n(1) Replace [cls] token with [det] token and\\nimage classification loss with bipartite matching\\nloss, (2)Propose a pre-trained Transformer object\\ndetection paradigm.\\n(1) Low detection accuracy,\\n(2) High computational\\noverhead.\\nUP-DETR (Dai et al., 2021a)\\n(1) Propose a new unsupervised pre-text task to\\nperform unsupervised pre-training on Transformer,\\n(2) Propose a patch detection reconstruction\\nbranch that is jointly optimized with patch\\ndetection.\\n(1) Slow convergence, (2)\\nPoor performance for small\\nobjects.\\nTransformer Backbone\\nFPT (Zhang et al., 2020)\\n(1) Propose a feature interaction method across\\nspace and scale, (2) High compatibility.\\n(1) Low detection accuracy,\\n(2) High computational\\noverhead.\\nPVT (Wang et al., 2021a)\\n(1) It can output multi-scale high-resolution\\nfeature maps; (2) The proposed spatial reduction\\nattention module makes PVT successfully applied\\nto dense prediction.\\n(1) High computational\\noverhead for high-resolution\\nimages; (2) Simple image\\ndivision loses the connection\\ninformation between different\\npatches.\\nSwin Transformer (Liu et al., 2021b)\\n(1) Hierarchical representation, (2) Introduced\\ncommunication between windows by computing\\nattention within shifted windows and reduced the\\ncomputational complexity to be linear with the\\nimage size.\\n(1) Excessive GPU memory\\nconsumption at higher image\\nresolutions, (2) Difficult to\\nretrain on small datasets, (3)\\nDifficult to transform\\npre-trained models at low\\nresolutions to higher\\nresolutions.\\nwithin the visual Transformer models. By exploring these aspects, we\\ncan potentially uncover novel optimization strategies and improve the\\nmodelâ€™s overall performance in various computer vision tasks.\\nThe inefficient image-sequence information transformation.\\nUnlike images, human-created languages have a high semantic density.\\nEach word in a sentence can be treated as high-dimensional semantic\\ninformation embedded in a low-dimensional vector representation.\\nHowever images, as a natural signal with high spatial redundancy,\\nhave a low information density per pixel. For example,\\nHe et al.\\n(2021) performed random high-scale masking of images, and then\\nreconstructed the images well with Decoder, demonstrating that much\\nhigher semantic features than pixel information density can be captured\\nin the images. But the current way of representing image information in\\nsequences using Transformer is not efficient enough, which can bring\\nabout accuracy degradation as well as high computational overhead.\\nEstablishing efficient transformations of image sequences can help\\nunlock the potential of the Transformer for CV tasks.\\n4.2. Future development outlook\\nVisual Transformer has made great progress in recent years, espe-\\ncially in object detection, and the performance has surpassed SOTA\\nCNN-based model on the COCO dataset. However, Transformer is not\\nmature enough in practical application deployment. For example, the\\ncomputational overhead is too large to be deployed on platforms with\\nlimited computer resources, and the real-time performance is not as\\ngood as the CNN-based one-stage approach.\\nSelf-supervised learning. While self-supervised learning has made\\na great success in natural language processing, current object detection\\nmodels, which are mainly supervised learning, require large amounts\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n15\\nY. Li et al.\\nof high-quality manually labeled data, which is usually too expensive.\\nTherefore, It is natural to think of using self-supervised learning for\\nvisual tasks in order to pre-training models using a large amount of\\ncheap data available on the Internet. For example, the MAE proposed\\nby He et al. (2021) uses masked self-encoders for self-supervised learn-\\ning, which are adequately pre-trained and then migrated to specific\\ntasks for fine-tuning.\\nLightweight Transformer. Since the performance of the current\\nTransformer-based detector is powerful enough, the development of a\\nlightweight Transformer architecture should be considered to broaden\\nits applicability. Key considerations could include a reduction in com-\\nputational demands, optimization for object detection, and intelligent\\nquery design to ensure high performance while minimizing computa-\\ntional overhead. This would enable deployment on mobile platforms\\nwith limited computational resources.\\nMultitasking. Within CNN-based methods, Mask R-CNN (He et al.,\\n2017) successfully performs instance segmentation alongside object\\ndetection, yielding superior results. Could a Transformer detector also\\nundertake multiple tasks simultaneously and derive benefits from this\\napproach? For instance, the performance of the object detector could be\\nenhanced by incorporating semantic segmentation. Semantic segmenta-\\ntion captures object boundaries, aiding object localization in detection,\\nand segments the background to delineate the contextual information\\nof the object, improving detection probability. Such an approach is\\nespecially useful as objects typically exist within specific contexts, such\\nas cars appearing on roads.\\n5. Conclusion\\nFor the past decade, CNN-based models have reigned supreme in\\nthe field of object detection. However, the Transformer has recently\\ndemonstrated superior performance and substantial potential in com-\\nputer vision (CV), rendering Transformer-based models a burgeoning\\nresearch topic within object detection. In this paper, we have conducted\\nan extensive review of mainstream Transformer-based object detectors\\ndeveloped over the past three years. Our focus has mainly been on\\ntheir concepts, innovative aspects, and detection accuracy. We have\\ncategorized these methods according to their model structure and es-\\ntablished a benchmark based on the COCO2017 dataset. Furthermore,\\nwe have conducted a multi-perspective analysis and comparison of\\nthese methods, summarizing their innovations and enhancements. We\\nhave also provided a comprehensive analysis of their limitations and\\nsummarized the existing challenges that persist within the application\\nof Transformer in object detection. This study aims to aid readers in\\ndeepening their understanding of Transformer object detectors, spark-\\ning research interest to unleash the potential of the Transformer model,\\nand enhancing its practical applications.\\nCRediT authorship contribution statement\\nYong Li: Writing â€“ review & editing, Supervision, Project admin-\\nistration. Naipeng Miao: Conceptualization, Methodology, Validation,\\nFormal analysis, Investigation, Writing- â€“ original draft, Writing â€“\\nreview & editing, Visualization. Liangdi Ma: Writing â€“ review & edit-\\ning. Feng Shuang: Funding acquisition, Resources. Xingwen Huang:\\nWriting â€“ review & editing.\\nDeclaration of competing interest\\nThe authors declare that they have no known competing finan-\\ncial interests or personal relationships that could have appeared to\\ninfluence the work reported in this paper.\\nData availability\\nThe data is public.\\nReferences\\nArkin, Ershat, Yadikar, Nurbiya, Muhtar, Yusnur, Ubul, Kurban, 2021. A survey of\\nobject detection based on CNN and transformer. In: 2021 IEEE 2nd International\\nConference on Pattern Recognition and Machine Learning (PRML). pp. 99â€“108.\\nhttp://dx.doi.org/10.1109/PRML52754.2021.9520732.\\nArkin, E., Yadikar, N., Xu, X., Aysa, A., Ubul, K., 2022. A survey: object detection\\nmethods from cnn to transformer. Multimedia Tools Appl. http://dx.doi.org/10.\\n1007/s11042-022-13801-3.\\nBai, Y., Mei, J., Yuille, A., Xie, C., 2021. Are transformers more robust than CNNs? http:\\n//dx.doi.org/10.48550/arXiv.2111.05464.\\nBochkovskiy, A., Wang, C.-Y., Liao, H.-Y.M., 2020. YOLOv4: Optimal speed and\\naccuracy of object detection.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakan-\\ntan, A., Shyam, P., Sastry, G., Askell, A., 2020. Language models are few-shot\\nlearners. Adv. Neural Inf. Process. Syst. 33, 1877â€“1901.\\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S., 2020.\\nEnd-to-end object detection with transformers.\\nChen, X., Ma, H., Wan, J., Li, B., Xia, T., 2017. Multi-view 3d object detection network\\nfor autonomous driving. In: Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition. pp. 1907â€“1915.\\nChen, T., Saxena, S., Li, L., Fleet, D.J., Hinton, G., 2021. Pix2seq: A language modeling\\nframework for object detection. arXiv:2109.10852[cs].\\nChen, C., Seff, A., Kornhauser, A., Xiao, J., 2015. Deepdriving: Learning affordance for\\ndirect perception in autonomous driving. In: Proceedings of the IEEE International\\nConference on Computer Vision. pp. 2722â€“2730.\\nChu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C., 2021.\\nTwins: Revisiting the design of spatial attention in vision transformers. arXiv:\\n2104.13840[cs].\\nCordonnier,\\nJ.-B.,\\nLoukas,\\nA.,\\nJaggi,\\nM.,\\n2020.\\nOn\\nthe\\nrelationship\\nbetween\\nself-attention and convolutional layers. arXiv:1911.03584[cs, stat].\\nDai, Z., Cai, B., Lin, Y., Chen, J., 2021a. UP-DETR: Unsupervised pre-training for object\\ndetection with transformers. arXiv:2011.09094[cs].\\nDai, X., Chen, Y., Xiao, B., Chen, D., Liu, M., Yuan, L., Zhang, L., 2021b. Dynamic head:\\nUnifying object detection heads with attentions. In: Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition. pp. 7373â€“7382.\\nDai, X., Chen, Y., Yang, J., Zhang, P., Yuan, L., Zhang, L., 2021c. Dynamic DETR: End-\\nto-end object detection with dynamic attention. In: 2021 IEEE/CVF International\\nConference on Computer Vision. ICCV, IEEE, Montreal, QC, Canada, pp. 2968â€“2977.\\nhttp://dx.doi.org/10.1109/ICCV48922.2021.00298.\\nDai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y., 2017. Deformable\\nconvolutional networks. In: Proceedings of the IEEE International Conference on\\nComputer Vision. pp. 764â€“773.\\nDevlin, J., Chang, M.-W., Lee, K., Toutanova, K., 2018. Bert: Pre-training of deep\\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.\\n04805.\\nDollar, P., Wojek, C., Schiele, B., Perona, P., 2012. Pedestrian detection: An evaluation\\nof the state of the art. IEEE Trans. Pattern Anal. Mach. Intell. 34 (4), 743â€“761,\\ndoi:10/bjsn5q.\\nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B., 2022. CSWin\\ntransformer: A general vision transformer backbone with cross-shaped windows.\\narXiv:2107.00652[cs].\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,\\nDehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N., 2021.\\nAn image is worth 16 Ã— 16 words: Transformers for image recognition at scale.\\narXiv:2010.11929[cs].\\nEveringham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., 2007. The\\nPASCAL visual object classes challenge 2007 (VOC2007) results. http://www.\\npascal-network.org/challenges/VOC/voc2007/workshop/index.html.\\nEveringham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., 2012. The\\nPASCAL visual object classes challenge 2012 (VOC2012) results. http://www.\\npascal-network.org/challenges/VOC/voc2012/workshop/index.html.\\nFang, Y., Liao, B., Wang, X., Fang, J., Qi, J., Wu, R., Niu, J., Liu, W., 2021. You only\\nlook at one sequence: Rethinking transformer in vision through object detection.\\narXiv:2106.00666[cs].\\nFelzenszwalb, P.F., Girshick, R.B., McAllester, D., Ramanan, D., 2010. Object detection\\nwith discriminatively trained part-based models. IEEE Trans. Pattern Anal. Mach.\\nIntell. 32 (9), 1627â€“1645, doi: 10/fgv7fd.\\nGao, P., Zheng, M., Wang, X., Dai, J., Li, H., 2021. Fast convergence of DETR with\\nspatially modulated co-attention. arXiv:2101.07448[cs].\\nGe, Z., Liu, S., Wang, F., Li, Z., Sun, J., 2021. YOLOX: Exceeding YOLO series in 2021.\\nGeirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.A., Brendel, W., 2019.\\nImageNet-trained CNNs are biased towards texture; increasing shape bias improves\\naccuracy and robustness. arXiv:1811.12231[cs, q-bio, stat].\\nGlorot, X., Bengio, Y., 2010. Understanding the difficulty of training deep feedforward\\nneural networks. In: Proceedings of the Thirteenth International Conference on Ar-\\ntificial Intelligence and Statistics. In: JMLR Workshop and Conference Proceedings,\\npp. 249â€“256.\\nEngineering Applications of Artificial Intelligence 126 (2023) 107021\\n16\\nY. Li et al.\\nHan, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A., Xu, C.,\\nXu, Y., Yang, Z., Zhang, Y., Tao, D., 2022. A survey on vision transformer.\\nIn: IEEE Transactions on Pattern Analysis and Machine Intelligence. p. 1. http:\\n//dx.doi.org/10.1109/TPAMI.2022.3152247.\\nHe, K., Chen, X., Xie, S., Li, Y., DollÃ¡r, P., Girshick, R., 2021. Masked autoencoders\\nare scalable vision learners.\\nHe, K., Gkioxari, G., DollÃ¡r, P., Girshick, R., 2017. Mask r-cnn. In: Proceedings of the\\nIEEE International Conference on Computer Vision. pp. 2961â€“2969.\\nHe, K., Zhang, X., Ren, S., Sun, J., 2015. Deep residual learning for image recognition.\\narXiv:1512.03385[cs].\\nKhan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M., 2021. Transformers\\nin vision: A survey. arXiv:2101.01169.\\nKobatake, H., Yoshinaga, Y., 1996. Detection of spicules on mammogram based on\\nskeleton analysis. IEEE Trans. Med. Imaging 15 (3), 235â€“245. http://dx.doi.org/\\n10.1109/42.500062.\\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R., 2019. Albert:\\nA lite bert for self-supervised learning of language representations. arXiv preprint\\narXiv:1909.11942.\\nLi, F., Zhang, H., Liu, S., Guo, J., Ni, L.M., Zhang, L., 2022. DN-DETR: Accelerate DETR\\ntraining by introducing query denoising. arXiv:2203.01305[cs].\\nLin, T.-Y., DollÃ¡r, P., Girshick, R., He, K., Hariharan, B., Belongie, S., 2016. Feature\\npyramid networks for object detection. arXiv:1612.03144.\\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., DollÃ¡r, P.,\\nZitnick, C.L., 2014. Microsoft coco: Common objects in context. In: European\\nConference on Computer Vision. Springer, pp. 740â€“755.\\nLin, J., Mao, X., Chen, Y., Xu, L., He, Y., Xue, H., 2022. D2ETR: Decoder-only DETR\\nwith computationally efficient cross-scale attention. arXiv:2203.00860[cs].\\nLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., Berg, A.C., 2016.\\nSSD: Single shot multibox detector. In: Computer Vision â€“ ECCV 2016. pp. 21â€“37.\\nhttp://dx.doi.org/10.1007/978-3-319-46448-0_2.\\nLiu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong, L.,\\nWei, F., Guo, B., 2021a. Swin transformer V2: Scaling up capacity and resolution.\\narXiv:2111.09883[cs].\\nLiu, S., Li, F., Zhang, H., Yang, X., Qi, X., Su, H., Zhu, J., Zhang, L., 2022a. DAB-DETR:\\nDynamic anchor boxes are better queries for DETR. arXiv:2201.12329[cs].\\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B., 2021b. Swin\\ntransformer: Hierarchical vision transformer using shifted windows. arXiv:2103.\\n14030[cs].\\nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., Xie, S., 2022b. A ConvNet\\nfor the 2020s. arXiv:2201.03545[cs].\\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettle-\\nmoyer, L., Stoyanov, V., 2019. RoBERTa: A robustly optimized BERT pretraining\\napproach. arXiv:1907.11692[cs].\\nLiu, Y., Zhang, Y., Wang, Y., Hou, F., Yuan, J., Tian, J., Zhang, Y., Shi, Z., Fan, J.,\\nHe, Z., 2021c. A survey of visual transformers. arXiv:2111.06091[cs].\\nLoshchilov, I., Hutter, F., 2017. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101.\\nMeng, D., Chen, X., Fan, Z., Zeng, G., Li, H., Yuan, Y., Sun, L., Wang, J., 2021.\\nConditional DETR for fast training convergence. In: Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision. pp. 3651â€“3660.\\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I., 2018. Improving language\\nunderstanding by generative pre-training.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., 2019. Language\\nmodels are unsupervised multitask learners. Openai Blog 1 (8), 9.\\nRajpurkar, P., Zhang, J., Lopyrev, K., Liang, P., 2016. SQuAD: 100, 000+ questions for\\nmachine comprehension of text. arXiv:1606.05250[cs].\\nRedmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. You only look once: Unified,\\nreal-time object detection. In: 2016 IEEE Conference on Computer Vision and\\nPattern Recognition. CVPR, IEEE, Las Vegas, NV, USA, pp. 779â€“788, doi:10/gc7rk9.\\nRedmon, J., Farhadi, A. and, 2017. YOLO9000: Better, faster, stronger. In: 2017 IEEE\\nConference on Computer Vision and Pattern Recognition. CVPR, pp. 6517â€“6525,\\nDOI: 10/gffdbj.\\nRedmon, J., Farhadi, A., 2018. YOLOv3: An incremental improvement.\\nRen, S., He, K., Girshick, R., Sun, J., 2016. Faster R-CNN: Towards Real-Time Object\\nDetection with region proposal networks. arXiv:1506.01497 [cs].\\nRoh, B., Shin, J., Shin, W., Kim, S., 2022. Sparse DETR: Efficient end-to-end object\\ndetection with learnable sparsity. arXiv:2111.14330[cs].\\nRoss, T.-Y., DollÃ¡r, G., 2017. Focal loss for dense object detection. In: Proceedings of\\nthe IEEE Conference on Computer Visionv and Pattern Recognition. pp. 2980â€“2988.\\nSang, E.F.T.K., De Meulder, F., 2003. Introduction to the CoNLL-2003 shared task:\\nLanguage-independent named entity recognition. arXiv:cs/0306050.\\nSun, Z., Cao, S., Yang, Y., Kitani, K.M., 2021. Rethinking transformer-based set\\nprediction for object detection. In: Proceedings of the IEEE/CVF International\\nConference on Computer Vision. pp. 3611â€“3620.\\nSung, K.-K., Poggio, T., 1998. Example-based learning for view-based human face\\ndetection. IEEE Trans. Pattern Anal. Mach. Intell. 20 (1), 39â€“51. http://dx.doi.\\norg/10.1109/34.655648/bnkgmt.\\nSutskever, I., Vinyals, O., Le, Q.V., 2014. Sequence to sequence learning with neural\\nnetworks. 9.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Å.,\\nPolosukhin, I., 2017. Attention is all you need. In: Advances in Neural Information\\nProcessing Systems, Vol. 30. Curran Associates, Inc.\\nWang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L., 2021a.\\nPyramid vision transforme: A versatile backbone for dense prediction without\\nconvolutions. arXiv:2102.12122[cs].\\nWang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L., 2022.\\nPVTv2: Improved baselines with pyramid vision transformer. arXiv:2106.13797[cs].\\nWang, T., Yuan, L., Chen, Y., Feng, J., Yan, S., 2021b. PnP-DETR: Towards efficient\\nvisual analysis with transformers. In: Proceedings of the IEEE/CVF International\\nConference on Computer Vision. pp. 4661â€“4670.\\nWang, Y., Zhang, X., Yang, T., Sun, J., 2021c. Anchor DETR: Query design for\\ntransformer-based object detection.\\nYang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., Gao, J., 2021. Focal self-attention\\nfor local-global interactions in vision transformers. arXiv:2107.00641[cs].\\nYao, Z., Ai, J., Li, B., Zhang, C., 2021. Efficient DETR: Improving end-to-end object\\ndetector with dense prior. arXiv:2104.01318[cs].\\nYuan, L., Chen, D., Chen, Y.-L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B.,\\nLi, C., Liu, C., Liu, M., Liu, Z., Lu, Y., Shi, Y., Wang, L., Wang, J., Xiao, B., Xiao, Z.,\\nYang, J., Zeng, M., Zhou, L., Zhang, P., 2021. Florence: A new foundation model\\nfor computer vision. arXiv:2111.11432[cs].\\nZhang, P., Dai, X., Yang, J., Xiao, B., Yuan, L., Zhang, L., Gao, J., 2021. Multi-scale\\nvision longformer: A new vision transformer for high-resolution image encoding.\\narXiv preprint arXiv:2103.15358.\\nZhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L.M., Shum, H.-Y., 2022a.\\nDINO: DETR with improved denoising anchor boxes for end-to-end object detection.\\narXiv:2203.03605[cs].\\nZhang, G., Luo, Z., Yu, Y., Cui, K., Lu, S., 2022b. Accelerating DETR convergence via\\nsemantic-aligned matching. arXiv:2203.06883[cs].\\nZhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., Sun, Q., 2020. Feature pyramid\\ntransformer. In: European Conference on Computer Vision. Springer, pp. 323â€“339.\\nZhao, Z.-Q., Zheng, P., Xu, S.-T., Wu, X., 2019. Object detection with deep learning: A\\nreview. IEEE Trans. Neural Netw. Learn. Syst. 30 (11), 3212â€“3232. http://dx.doi.\\norg/10.1109/TNNLS.2018.2876865.\\nZheng, M., Gao, P., Zhang, R., Li, K., Wang, X., Li, H., Dong, H., 2021. End-to-end\\nobject detection with adaptive clustering transformer. arXiv:2011.09315[cs].\\nZhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J., 2021. Deformable DETR: Deformable\\ntransformers for end-to-end object detection. arXiv:2010.04159[cs].\\n', metadata={'source': 'main.pdf'}, embedding=None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = Parser()\n",
    "file_path = \"main.pdf\"\n",
    "documents = parser.parse(file_path)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunk the parsed text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = SentenceChunker()\n",
    "chunked_documents = [Document(content=chunk) for chunk in chunker.chunk_text(documents[0].content)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize Vector Store and add chunked documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = TfidfVectorStore()\n",
    "vector_store.add_documents(chunked_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up the LLM and system context for RagAgent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "llm = LLM(api_key=API_KEY)\n",
    "system_context = SystemMessage(content=\"Your name is PDF reading assistant. You are to help readers with accurate informations\")\n",
    "conversation = MaxSystemContextConversation(system_context=system_context, max_size=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize RagAgent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RagAgent(llm=llm, conversation=conversation, system_context=system_context, vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RagAgent Resource Type: Agent\n"
     ]
    }
   ],
   "source": [
    "# Check agent resource type\n",
    "print(\"RagAgent Resource Type:\", agent.resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Transformers and convolutional networks are two distinct deep learning architectures with unique strengths and weaknesses:**\\n\\n**Transformers:**\\n\\n* **Attention-based:** Uses self-attention and cross-attention between token pairs to capture long-range dependencies.\\n* **Sequence-to-sequence learning:** Designed for sequence-to-sequence tasks, such as machine translation, text summarization, and language modeling.\\n* **Parallel processing:** Can process entire sequences in parallel, leading to faster training and inference.\\n* **Non-sequential:** Does not require data to be presented in a specific order.\\n* **Memory-efficient:** Uses attention to capture dependencies without storing the entire sequence in memory.\\n\\n\\n**Convolutional Networks:**\\n\\n* **Spatial feature extraction:** Uses convolution filters to extract spatial features from data, such as images and time series.\\n* **Local processing:** Processes data locally, considering only a small neighborhood of each input.\\n* **Feature extraction:** Excellent at extracting local features and identifying patterns in spatial data.\\n* **Efficient for image recognition:** Widely used for tasks such as object recognition, image classification, and image generation.\\n\\n\\n**Key Differences:**\\n\\n**1. Architecture:**\\n- Transformers are based on attention mechanisms, while convolutional networks rely on convolution filters.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.exec(\"different between transformers and convultionary networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Key Points:**\n",
    "\n",
    "GroqModel is initialized and passed to the RAG agent as the LLM (llm).\n",
    "The RAG agent will first retrieve relevant chunks from the vector store using the query and then pass these chunks to the Groq LLM to generate a refined response.\n",
    "By integrating GroqModel, the agent becomes capable of both retrieving relevant information and enhancing it through natural language generation.\n",
    "\n",
    "# **Conclusion:**\n",
    "\n",
    "With the integration of the GroqModel, we have successfully built a RAG agent  that parses, chunks, and retrieves information from a PDF, while generating rich responses using an LLM. This setup can handle complex queries, blending document search and language generation for more comprehensive answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTEBOOK METADATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Dominion John \n",
      "GitHub Username: DOMINION-JOHN1\n",
      "Last Modified: 2024-10-23 09:39:18.741447\n",
      "Platform: Windows 11\n",
      "Python Version: 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n",
      "Swarmauri Version: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Display author information\n",
    "author_name = \"Dominion John \" \n",
    "github_username = \"DOMINION-JOHN1\"  \n",
    "\n",
    "print(f\"Author: {author_name}\")\n",
    "print(f\"GitHub Username: {github_username}\")\n",
    "\n",
    "# Last modified datetime (file's metadata)\n",
    "notebook_file = \"Notebook 1 Development and Implementation.ipynb\"\n",
    "try:\n",
    "    last_modified_time = os.path.getmtime(notebook_file)\n",
    "    last_modified_datetime = datetime.fromtimestamp(last_modified_time)\n",
    "    print(f\"Last Modified: {last_modified_datetime}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve last modified datetime: {e}\")\n",
    "\n",
    "# Display platform, Python version, and Swarmauri version\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "# Checking Swarmauri version\n",
    "try:\n",
    "    import swarmauri\n",
    "    print(f\"Swarmauri Version: {swarmauri.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Swarmauri is not installed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri(0.5.0)",
   "language": "python",
   "name": "swarmauri-0.5.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
