{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aeefb54-392c-4851-8604-b643181ca1d5",
   "metadata": {},
   "source": [
    "# Notebook 03: Building the RAG Agent\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will focus on constructing a fully functional Retrieval-Augmented Generation (RAG) Agent. The RAG agent combines various componentsâ€”like the language model (LLM), vector store, and conversation contextâ€”into a unified pipeline to retrieve relevant information from external documents and generate contextually enriched responses.\n",
    "\n",
    "We will walk through the final steps of assembling the RAG agent and demonstrate how to execute queries using the agent.\n",
    "\n",
    "## Review of Components\n",
    "\n",
    "Before diving into the full implementation of the RAG agent, letâ€™s briefly recap the key components:\n",
    "\n",
    "- **Language Model (LLM)**: Generates responses based on the input query.\n",
    "- **Vector Store**: Holds external documents and allows retrieval of relevant documents based on the query.\n",
    "- **Conversation Context**: Maintains the dialogue context, ensuring that responses are coherent and relevant to previous interactions.\n",
    "\n",
    "In this notebook, we will integrate these components into the RagAgent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42e960-8b6f-4a15-97a8-616e2505c6ae",
   "metadata": {},
   "source": [
    "## Setting Up the Vector Store and Adding Documents\n",
    "\n",
    "The first step is to set up the vector store, which will hold our external knowledge in the form of documents. The RAG agent will use this store to retrieve relevant information during query processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8172c2a-4c1d-4000-b66d-e5705c334b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.documents.concrete.Document import Document\n",
    "from swarmauri.vector_stores.concrete.TfidfVectorStore import TfidfVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2eeaa933-20ad-4e29-ac25-bb6b67e65737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vector store\n",
    "vector_store = TfidfVectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9bc0860c-079b-4237-952c-bc4e9672ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents containing knowledge\n",
    "documents = [\n",
    "    Document(content=\"Their sister's name is Jane.\"),\n",
    "    Document(content=\"Their mother's name is Jean.\"),\n",
    "    Document(content=\"Their father's name is Joseph.\"),\n",
    "    Document(content=\"Their grandfather's name is Alex.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81478a7e-48b2-4d97-bb60-cb26091a8733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to the vector store\n",
    "vector_store.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a5577a77-5c58-49b1-8606-672de5059dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents added to the vector store.\n"
     ]
    }
   ],
   "source": [
    "# Verify the documents have been added\n",
    "print(f\"{len(vector_store.documents)} documents added to the vector store.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52675a5-cc0e-4391-a8ee-a1f974080cb8",
   "metadata": {},
   "source": [
    "## Configuring the Conversation Context\n",
    "\n",
    "Next, we set up the conversation context to ensure the agent can handle multi-turn dialogues. This context helps the agent remember previous user queries and system responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9426f8cd-691f-40f5-8891-9b4877079cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.conversations.concrete.MaxSystemContextConversation import MaxSystemContextConversation\n",
    "from swarmauri.messages.concrete.SystemMessage import SystemMessage\n",
    "from swarmauri.messages.concrete.HumanMessage import HumanMessage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f9fe2e1-93f4-484e-aaec-c33bd7f88885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a system message\n",
    "system_context = SystemMessage(content=\"Your name is Jeff.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee557d5c-2ab5-47e5-9f69-008fabaad907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the conversation\n",
    "conversation = MaxSystemContextConversation(system_context=system_context, max_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1bfea25b-618d-43ab-a4f4-786bae38456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a user message\n",
    "user_message = HumanMessage(content=\"What is my name?\")\n",
    "conversation.add_message(user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a366f361-64f7-4cd7-931b-d50791d1fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current conversation history:\n",
      "Your name is Jeff.\n",
      "What is my name?\n"
     ]
    }
   ],
   "source": [
    "# Print the current conversation context\n",
    "print(\"Current conversation history:\")\n",
    "for message in conversation.history:\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc6461d-5b47-47b3-88e9-6bc5ccb24204",
   "metadata": {},
   "source": [
    "## Integrating the Language Model (LLM)\n",
    "\n",
    "The GroqModel (our chosen LLM) will generate responses based on both the retrieved documents and the conversation context. Let's initialize the LLM and integrate it into our RAG agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2afe2d-c8de-417a-8fb2-448718a4b3a5",
   "metadata": {},
   "source": [
    "### Importing Libraries and Loading Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a8ce951-8451-49d8-8ae5-833b4ab1bb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from swarmauri.llms.concrete.GroqModel import GroqModel as LLM\n",
    "from swarmauri.conversations.concrete.Conversation import Conversation\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877d480-e0bd-497b-acd7-c4032a359c56",
   "metadata": {},
   "source": [
    "### Setting the API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9264b46c-ac9f-4560-b68e-f178e2beac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Check if the API key is set\n",
    "if not API_KEY:\n",
    "    print(\"API key is not set. Please set the GROQ_API_KEY environment variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d45591-bfb2-4096-9af8-c67b06982c95",
   "metadata": {},
   "source": [
    "### Function to Get Allowed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c89df189-fe1a-4843-a1cb-de1005c25867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get allowed models, filtering out failing ones\n",
    "def get_allowed_models(llm):\n",
    "    failing_llms = [\n",
    "        \"llama3-70b-8192\",\n",
    "        \"llama-3.2-90b-text-preview\",\n",
    "        \"mixtral-8x7b-32768\",\n",
    "        \"llava-v1.5-7b-4096-preview\",\n",
    "        \"llama-guard-3-8b\",\n",
    "    ]\n",
    "    return [model for model in llm.allowed_models if model not in failing_llms]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a5ade4-e6fb-4821-94d0-5fe4dcbfd311",
   "metadata": {},
   "source": [
    "### Initializing the GroqModel and Displaying Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8a4385a0-56e3-4bc9-ae96-fb377b3b1585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GroqModel\n",
    "llm = LLM(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dbdac06d-1e2c-414f-9edb-17f0b4b1e2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource: LLM\n",
      "Type: GroqModel\n",
      "Default Name: gemma-7b-it\n"
     ]
    }
   ],
   "source": [
    "# Print model information\n",
    "print(f\"Resource: {llm.resource}\")\n",
    "print(f\"Type: {llm.type}\")\n",
    "print(f\"Default Name: {llm.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cc9e7-ea8e-4f17-baeb-fb7b4998e809",
   "metadata": {},
   "source": [
    "### Retrieving Allowed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b53af4ff-9948-4c6c-b5f4-aa9845e8a2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed Models: ['gemma-7b-it', 'gemma2-9b-it', 'llama-3.1-70b-versatile', 'llama-3.1-8b-instant', 'llama-3.2-11b-text-preview', 'llama-3.2-1b-preview', 'llama-3.2-3b-preview', 'llama3-8b-8192', 'llama3-groq-70b-8192-tool-use-preview', 'llama3-groq-8b-8192-tool-use-preview']\n"
     ]
    }
   ],
   "source": [
    "# Get allowed models\n",
    "allowed_models = get_allowed_models(llm)\n",
    "print(\"Allowed Models:\", allowed_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d498e2-824f-49f4-951d-de420d45bffe",
   "metadata": {},
   "source": [
    "### Example Usage with No System Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6b3d9be-f0b1-4ef6-b76d-59b3d3dbea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name to the first available allowed model\n",
    "llm.name = allowed_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b45da1b4-2df0-4e7a-8839-386f9babb734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conversation\n",
    "conversation = Conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "13f3ab45-aae2-462b-b4db-88392e97f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a human message\n",
    "input_data = \"Hello\"\n",
    "human_message = HumanMessage(content=input_data)\n",
    "conversation.add_message(human_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e4274715-de2f-4b86-96a1-3ef3bdb344d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with no system context for gemma-7b-it: Hello! ðŸ‘‹ It's great to hear from you. How can I help you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# Predict response\n",
    "llm.predict(conversation=conversation)\n",
    "prediction = conversation.get_last().content\n",
    "print(f\"Prediction with no system context for {llm.name}: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7250363-2ed2-4a32-83f1-2e10bf310b63",
   "metadata": {},
   "source": [
    "### Example Usage with a System Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21f6b682-dc90-4ce0-934a-5ab575870fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with a system context\n",
    "system_context = 'As assistant that provide answers to questions.'\n",
    "conversation = MaxSystemContextConversation(system_context=SystemMessage(content=system_context), max_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce81283a-eb82-4c65-a80a-9c68658c92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a human message with the content \"Hi\" and add it to the conversation\n",
    "human_message = HumanMessage(content=\"Hi\")\n",
    "conversation.add_message(human_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b818ce52-cb03-49ea-9382-b85ee44c3092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with system context for gemma-7b-it: Certainly! I am ready to assist you with any questions you may have. Feel free to ask me anything you like, and I will do my best to provide accurate and helpful information. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Predict response\n",
    "llm.predict(conversation=conversation)\n",
    "prediction = conversation.get_last().content\n",
    "print(f\"Prediction with system context for {llm.name}: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d756f9-b156-4100-b454-e93a0b3b82e8",
   "metadata": {},
   "source": [
    "## Building the RAG Agent\n",
    "Finally, we will assemble all the components into the RagAgent. The agent will retrieve relevant documents from the vector store and use the language model to generate responses based on both the retrieved content and the conversation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9bcbf57-b686-460e-8a47-90e9ddd817c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.agents.concrete.RagAgent import RagAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13e5a9ca-4bbf-4da9-a116-d7afd9ef94b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG Agent by combining LLM, conversation, and vector store\n",
    "rag_agent = RagAgent(\n",
    "    llm=llm,\n",
    "    conversation=conversation,\n",
    "    system_context=system_context,\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db6f650c-c6c5-4fc9-8e0b-0a17a896b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query to the RAG agent\n",
    "query = \"What is the name of their grandfather?\"\n",
    "response = rag_agent.exec(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2acc5132-255d-44fc-a46b-6331b5f2e21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Agent Response: \n"
     ]
    }
   ],
   "source": [
    "# Print the agent's response\n",
    "print(f\"RAG Agent Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f83af-4aa8-45a7-a11e-933a30d2b816",
   "metadata": {},
   "source": [
    "## Handling Queries with the RAG Agent\n",
    "Now that the RAG agent is fully configured, we can test it with various queries. The RAG agent will retrieve documents from the vector store, interpret the conversation context, and generate informed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "42100589-ada6-4616-b953-73a514f1c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with different queries\n",
    "queries = [\n",
    "    \"What is the name of their mother?\",\n",
    "    \"Tell me more about their family.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0e0b5c94-f506-4aec-8be4-ae662dfefe57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the name of their mother?\n",
      "RAG Agent Response: The name of their grandfather is Alex, as mentioned in the given information.\n",
      "\n",
      "Query: Tell me more about their family.\n",
      "RAG Agent Response:  The provided text does not contain any information regarding the name of their mother, so I am unable to answer this question from the given context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    response = rag_agent.exec(query)\n",
    "    print(f\"Query: {query}\\nRAG Agent Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf92cacc-6f62-49be-a69f-ad7ea6297b31",
   "metadata": {},
   "source": [
    "## Notebook Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1839c758-4b94-45e5-accf-b5aea7c7a4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Huzaifa Irshad \n",
      "GitHub Username: irshadhuzaifa\n",
      "Last Modified: 2024-10-28 16:36:19.087858\n",
      "Platform: Windows 11\n",
      "Python Version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "Swarmauri Version: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "author_name = \"Huzaifa Irshad \" \n",
    "github_username = \"irshadhuzaifa\"\n",
    "\n",
    "print(f\"Author: {author_name}\")\n",
    "print(f\"GitHub Username: {github_username}\")\n",
    "\n",
    "notebook_file = \"Notebook_03_Building_RAG_Agent.ipynb\"\n",
    "try:\n",
    "    last_modified_time = os.path.getmtime(notebook_file)\n",
    "    last_modified_datetime = datetime.fromtimestamp(last_modified_time)\n",
    "    print(f\"Last Modified: {last_modified_datetime}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve last modified datetime: {e}\")\n",
    "\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import swarmauri\n",
    "    print(f\"Swarmauri Version: {swarmauri.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Swarmauri is not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0785d52-d6d4-4d89-89ed-5f4fd4f6e4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri(0.5)",
   "language": "python",
   "name": "swarmauri-0.5.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
