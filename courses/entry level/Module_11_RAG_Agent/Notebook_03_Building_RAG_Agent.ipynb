{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aeefb54-392c-4851-8604-b643181ca1d5",
   "metadata": {},
   "source": [
    "# Notebook 03: Building the RAG Agent\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will focus on constructing a fully functional Retrieval-Augmented Generation (RAG) Agent. The RAG agent combines various componentsâ€”like the language model (LLM), vector store, and conversation contextâ€”into a unified pipeline to retrieve relevant information from external documents and generate contextually enriched responses.\n",
    "\n",
    "We will walk through the final steps of assembling the RAG agent and demonstrate how to execute queries using the agent.\n",
    "\n",
    "## Review of Components\n",
    "\n",
    "Before diving into the full implementation of the RAG agent, letâ€™s briefly recap the key components:\n",
    "\n",
    "- **Language Model (LLM)**: Generates responses based on the input query.\n",
    "- **Vector Store**: Holds external documents and allows retrieval of relevant documents based on the query.\n",
    "- **Conversation Context**: Maintains the dialogue context, ensuring that responses are coherent and relevant to previous interactions.\n",
    "\n",
    "In this notebook, we will integrate these components into the RagAgent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42e960-8b6f-4a15-97a8-616e2505c6ae",
   "metadata": {},
   "source": [
    "## Setting Up the Vector Store and Adding Documents\n",
    "\n",
    "The first step is to set up the vector store, which will hold our external knowledge in the form of documents. The RAG agent will use this store to retrieve relevant information during query processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bb924ff-6a9b-49e9-b471-979226690a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents added to the vector store.\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.documents.concrete.Document import Document\n",
    "from swarmauri.vector_stores.concrete.TfidfVectorStore import TfidfVectorStore\n",
    "\n",
    "# Initialize the vector store\n",
    "vector_store = TfidfVectorStore()\n",
    "\n",
    "# Sample documents containing knowledge\n",
    "documents = [\n",
    "    Document(content=\"Their sister's name is Jane.\"),\n",
    "    Document(content=\"Their mother's name is Jean.\"),\n",
    "    Document(content=\"Their father's name is Joseph.\"),\n",
    "    Document(content=\"Their grandfather's name is Alex.\"),\n",
    "]\n",
    "\n",
    "# Add documents to the vector store\n",
    "vector_store.add_documents(documents)\n",
    "\n",
    "# Verify the documents have been added\n",
    "print(f\"{len(vector_store.documents)} documents added to the vector store.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52675a5-cc0e-4391-a8ee-a1f974080cb8",
   "metadata": {},
   "source": [
    "## Configuring the Conversation Context\n",
    "\n",
    "Next, we set up the conversation context to ensure the agent can handle multi-turn dialogues. This context helps the agent remember previous user queries and system responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "880404fb-d0b0-499f-a7fc-56d630ba1869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current conversation history:\n",
      "Your name is Jeff.\n",
      "What is my name?\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.conversations.concrete.MaxSystemContextConversation import MaxSystemContextConversation\n",
    "from swarmauri.messages.concrete.SystemMessage import SystemMessage\n",
    "from swarmauri.messages.concrete.HumanMessage import HumanMessage \n",
    "\n",
    "# Create a system message\n",
    "system_context = SystemMessage(content=\"Your name is Jeff.\")\n",
    "\n",
    "# Initialize the conversation\n",
    "conversation = MaxSystemContextConversation(system_context=system_context, max_size=4)\n",
    "\n",
    "# Add a user message\n",
    "user_message = HumanMessage(content=\"What is my name?\")\n",
    "conversation.add_message(user_message)\n",
    "\n",
    "# Print the current conversation context\n",
    "print(\"Current conversation history:\")\n",
    "for message in conversation.history:\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc6461d-5b47-47b3-88e9-6bc5ccb24204",
   "metadata": {},
   "source": [
    "## Integrating the Language Model (LLM)\n",
    "\n",
    "The GroqModel (our chosen LLM) will generate responses based on both the retrieved documents and the conversation context. Let's initialize the LLM and integrate it into our RAG agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2afe2d-c8de-417a-8fb2-448718a4b3a5",
   "metadata": {},
   "source": [
    "### Importing Libraries and Loading Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a8ce951-8451-49d8-8ae5-833b4ab1bb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from swarmauri.llms.concrete.GroqModel import GroqModel as LLM\n",
    "from swarmauri.conversations.concrete.Conversation import Conversation\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877d480-e0bd-497b-acd7-c4032a359c56",
   "metadata": {},
   "source": [
    "### Setting the API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9264b46c-ac9f-4560-b68e-f178e2beac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Check if the API key is set\n",
    "if not API_KEY:\n",
    "    print(\"API key is not set. Please set the GROQ_API_KEY environment variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d45591-bfb2-4096-9af8-c67b06982c95",
   "metadata": {},
   "source": [
    "### Function to Get Allowed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c89df189-fe1a-4843-a1cb-de1005c25867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get allowed models, filtering out failing ones\n",
    "def get_allowed_models(llm):\n",
    "    failing_llms = [\n",
    "        \"llama3-70b-8192\",\n",
    "        \"llama-3.2-90b-text-preview\",\n",
    "        \"mixtral-8x7b-32768\",\n",
    "        \"llava-v1.5-7b-4096-preview\",\n",
    "        \"llama-guard-3-8b\",\n",
    "    ]\n",
    "    return [model for model in llm.allowed_models if model not in failing_llms]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a5ade4-e6fb-4821-94d0-5fe4dcbfd311",
   "metadata": {},
   "source": [
    "### Initializing the GroqModel and Displaying Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbdac06d-1e2c-414f-9edb-17f0b4b1e2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource: LLM\n",
      "Type: GroqModel\n",
      "Default Name: gemma-7b-it\n"
     ]
    }
   ],
   "source": [
    "# Initialize the GroqModel\n",
    "llm = LLM(api_key=API_KEY)\n",
    "\n",
    "# Print model information\n",
    "print(f\"Resource: {llm.resource}\")\n",
    "print(f\"Type: {llm.type}\")\n",
    "print(f\"Default Name: {llm.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cc9e7-ea8e-4f17-baeb-fb7b4998e809",
   "metadata": {},
   "source": [
    "### Retrieving Allowed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b53af4ff-9948-4c6c-b5f4-aa9845e8a2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed Models: ['gemma-7b-it', 'gemma2-9b-it', 'llama-3.1-70b-versatile', 'llama-3.1-8b-instant', 'llama-3.2-11b-text-preview', 'llama-3.2-1b-preview', 'llama-3.2-3b-preview', 'llama3-8b-8192', 'llama3-groq-70b-8192-tool-use-preview', 'llama3-groq-8b-8192-tool-use-preview']\n"
     ]
    }
   ],
   "source": [
    "# Get allowed models\n",
    "allowed_models = get_allowed_models(llm)\n",
    "print(\"Allowed Models:\", allowed_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d498e2-824f-49f4-951d-de420d45bffe",
   "metadata": {},
   "source": [
    "### Example Usage with No System Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b997fb8a-3a14-4de6-8bb3-2ebbb9f3b2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with no system context for gemma-7b-it: Hello! ðŸ‘‹ It's great to hear from you. What can I do for you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# Example usage with no system context\n",
    "llm.name = allowed_models[0]\n",
    "\n",
    "# Create a conversation\n",
    "conversation = Conversation()\n",
    "\n",
    "# Add a human message\n",
    "input_data = \"Hello\"\n",
    "human_message = HumanMessage(content=input_data)\n",
    "conversation.add_message(human_message)\n",
    "\n",
    "# Predict response\n",
    "llm.predict(conversation=conversation)\n",
    "prediction = conversation.get_last().content\n",
    "print(f\"Prediction with no system context for {llm.name}: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7250363-2ed2-4a32-83f1-2e10bf310b63",
   "metadata": {},
   "source": [
    "### Example Usage with a System Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af56c215-d5b8-468f-944d-6be46c5fc4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with system context for gemma-7b-it: **Human History (Human1)**\n",
      "\n",
      "**Unit Overview:**\n",
      "\n",
      "Human history explores the remarkable journey of our species from our origins in Africa to the diverse civilizations that have shaped the world. This unit examines the key events, developments, and cultural transformations that have defined human existence.\n",
      "\n",
      "**Topics Covered:**\n",
      "\n",
      "**Part 1: Origins and Evolution**\n",
      "\n",
      "* Paleolithic Era: Stone tools, human evolution, and the first civilizations\n",
      "* Neolithic Revolution: Agricultural innovations and the rise of settled life\n",
      "* The Rise of Complex Societies: Early cities and states in Mesopotamia, Egypt, and the Indus Valley\n",
      "\n",
      "**Part 2: Classical Eras**\n",
      "\n",
      "* Ancient Greece: Political systems, philosophy, and the development of democracy\n",
      "* Roman Republic and Empire: Political and military achievements, infrastructure, and literary contributions\n",
      "* China: Imperial expansion, philosophical advancements, and technological innovations\n",
      "\n",
      "**Part 3: Medieval and Renaissance**\n",
      "\n",
      "* The Middle Ages: Feudalism, religious conflicts, and the rise of Christianity and Islam\n",
      "* The Renaissance: Cultural and intellectual revival, rediscovery of classical learning, and the emergence of humanism\n",
      "\n",
      "**Part 4: Modern Era**\n",
      "\n",
      "* The Age of Exploration: European exploration, colonization, and the Columbian Exchange\n",
      "* The Enlightenment\n"
     ]
    }
   ],
   "source": [
    "# Example usage with a system context\n",
    "system_context = 'Human History'\n",
    "conversation = MaxSystemContextConversation(system_context=SystemMessage(content=system_context), max_size=2)\n",
    "system_message = SystemMessage(content=system_context)\n",
    "conversation.add_message(HumanMessage(content=\"human1\"))\n",
    "\n",
    "human_message = HumanMessage(content=\"Hi\")\n",
    "conversation.add_message(human_message)\n",
    "\n",
    "# Predict response\n",
    "llm.predict(conversation=conversation)\n",
    "prediction = conversation.get_last().content\n",
    "print(f\"Prediction with system context for {llm.name}: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c4ed0-40b9-4d07-be06-eabb67f0ff4f",
   "metadata": {},
   "source": [
    "### Complete Script for Initializing, Configuring, and Using GroqModel Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c2e906e-ef08-4344-9040-f0d36e75ad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource: LLM\n",
      "Type: GroqModel\n",
      "Default Name: gemma-7b-it\n",
      "Allowed Models: ['gemma-7b-it', 'gemma2-9b-it', 'llama-3.1-70b-versatile', 'llama-3.1-8b-instant', 'llama-3.2-11b-text-preview', 'llama-3.2-1b-preview', 'llama-3.2-3b-preview', 'llama3-8b-8192', 'llama3-groq-70b-8192-tool-use-preview', 'llama3-groq-8b-8192-tool-use-preview']\n",
      "Prediction with no system context for gemma-7b-it: Hello! ðŸ‘‹ I'm happy to hear from you. How can I help you today? ðŸ˜Š\n",
      "Prediction with system context for gemma-7b-it: **Human History (Human1)**\n",
      "\n",
      "**Unit 1: Origins and Evolution**\n",
      "\n",
      "* Origin and evolution of hominin species\n",
      "* The Stone Age: Paleolithic and Mesolithic eras\n",
      "* The development of agriculture and the transition to a sedentary lifestyle\n",
      "\n",
      "**Unit 2: Ancient Civilizations**\n",
      "\n",
      "* The rise and fall of ancient civilizations such as Mesopotamia, Egypt, and the Indus Valley Civilization\n",
      "* The development of writing, technology, and art\n",
      "* The impact of ancient civilizations on the modern world\n",
      "\n",
      "**Unit 3: Classical Era**\n",
      "\n",
      "* The rise of Greece and Rome\n",
      "* The fall of the Roman Empire and the Middle Ages\n",
      "* The development of Christianity, Islam, and Judaism\n",
      "\n",
      "**Unit 4: The Middle Ages**\n",
      "\n",
      "* The rise of feudalism and the medieval state\n",
      "* The Black Death and other major epidemics\n",
      "* The Crusades and the Islamic Golden Age\n",
      "\n",
      "**Unit 5: The Renaissance and Reformation**\n",
      "\n",
      "* The rebirth of classical learning and the rise of individualism\n",
      "* The Protestant Reformation and the rise of nation-states\n",
      "* The Age of Exploration and the Age of Discovery\n",
      "\n",
      "**Unit 6: The Modern Era**\n",
      "\n",
      "* The Scientific Revolution and the Industrial Revolution\n",
      "* The rise of nationalism and\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from swarmauri.llms.concrete.GroqModel import GroqModel as LLM\n",
    "from swarmauri.conversations.concrete.Conversation import Conversation\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "# Check if the API key is set\n",
    "if not API_KEY:\n",
    "    print(\"API key is not set. Please set the GROQ_API_KEY environment variable.\")\n",
    "\n",
    "# Function to get allowed models, filtering out failing ones\n",
    "def get_allowed_models(llm):\n",
    "    failing_llms = [\n",
    "        \"llama3-70b-8192\",\n",
    "        \"llama-3.2-90b-text-preview\",\n",
    "        \"mixtral-8x7b-32768\",\n",
    "        \"llava-v1.5-7b-4096-preview\",\n",
    "        \"llama-guard-3-8b\",\n",
    "    ]\n",
    "    return [model for model in llm.allowed_models if model not in failing_llms]\n",
    "\n",
    "# Initialize the GroqModel\n",
    "llm = LLM(api_key=API_KEY)\n",
    "\n",
    "# Print model information\n",
    "print(f\"Resource: {llm.resource}\")\n",
    "print(f\"Type: {llm.type}\")\n",
    "print(f\"Default Name: {llm.name}\")\n",
    "\n",
    "# Get allowed models\n",
    "allowed_models = get_allowed_models(llm)\n",
    "print(\"Allowed Models:\", allowed_models)\n",
    "\n",
    "# Example usage with no system context\n",
    "\n",
    "llm.name = allowed_models[0]\n",
    "\n",
    "# Create a conversation\n",
    "conversation = Conversation()\n",
    "\n",
    "# Add a human message\n",
    "input_data = \"Hello\"\n",
    "human_message = HumanMessage(content=input_data)\n",
    "conversation.add_message(human_message)\n",
    "\n",
    "# Predict response\n",
    "llm.predict(conversation=conversation)\n",
    "prediction = conversation.get_last().content\n",
    "print(f\"Prediction with no system context for {llm.name}: {prediction}\")\n",
    "\n",
    "# Example usage with a system context\n",
    "system_context = 'Human History'\n",
    "conversation = MaxSystemContextConversation(system_context=SystemMessage(content=system_context), max_size=2)\n",
    "system_message = SystemMessage(content=system_context)\n",
    "conversation.add_message(HumanMessage(content=\"human1\"))\n",
    "\n",
    "human_message = HumanMessage(content=\"Hi\")\n",
    "conversation.add_message(human_message)\n",
    "\n",
    "# Predict response\n",
    "llm.predict(conversation=conversation)\n",
    "prediction = conversation.get_last().content\n",
    "print(f\"Prediction with system context for {llm.name}: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d756f9-b156-4100-b454-e93a0b3b82e8",
   "metadata": {},
   "source": [
    "## Building the RAG Agent\n",
    "Finally, we will assemble all the components into the RagAgent. The agent will retrieve relevant documents from the vector store and use the language model to generate responses based on both the retrieved content and the conversation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9355ccd-0187-4f3d-9188-3c6596a084c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Agent Response: The name of their grandfather is Alex, as stated in the given sentence: \"Their grandfather's name is Alex.\"\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.agents.concrete.RagAgent import RagAgent\n",
    "\n",
    "# Initialize the RAG Agent by combining LLM, conversation, and vector store\n",
    "rag_agent = RagAgent(\n",
    "    llm=llm,\n",
    "    conversation=conversation,\n",
    "    system_context=system_context,\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "# Example query to the RAG agent\n",
    "query = \"What is the name of their grandfather?\"\n",
    "response = rag_agent.exec(query)\n",
    "\n",
    "# Print the agent's response\n",
    "print(f\"RAG Agent Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f83af-4aa8-45a7-a11e-933a30d2b816",
   "metadata": {},
   "source": [
    "## Handling Queries with the RAG Agent\n",
    "Now that the RAG agent is fully configured, we can test it with various queries. The RAG agent will retrieve documents from the vector store, interpret the conversation context, and generate informed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db2657fc-8673-4f5e-8721-45614c093b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the name of their mother?\n",
      "RAG Agent Response: The name of their mother is Jean, as mentioned in the given information.\n",
      "\n",
      "Query: What is the name of their sister?\n",
      "RAG Agent Response: The name of their sister is Jane, as stated in the given sentence: \"Their sister's name is Jane.\"\n",
      "\n",
      "Query: Tell me more about their family.\n",
      "RAG Agent Response: The provided text does not contain any further information about the individual's family beyond the names of their mother, father, grandfather, and sister. Therefore, I am unable to provide any additional details about their family structure, dynamics, or relationships.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the agent with different queries\n",
    "queries = [\n",
    "    \"What is the name of their mother?\",\n",
    "    \"What is the name of their sister?\",\n",
    "    \"Tell me more about their family.\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    response = rag_agent.exec(query)\n",
    "    print(f\"Query: {query}\\nRAG Agent Response: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf92cacc-6f62-49be-a69f-ad7ea6297b31",
   "metadata": {},
   "source": [
    "## Notebook Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1839c758-4b94-45e5-accf-b5aea7c7a4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Huzaifa Irshad \n",
      "GitHub Username: irshadhuzaifa\n",
      "Last Modified: 2024-10-28 13:59:29.671765\n",
      "Platform: Windows 11\n",
      "Python Version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "Swarmauri Version: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "author_name = \"Huzaifa Irshad \" \n",
    "github_username = \"irshadhuzaifa\"\n",
    "\n",
    "print(f\"Author: {author_name}\")\n",
    "print(f\"GitHub Username: {github_username}\")\n",
    "\n",
    "notebook_file = \"Notebook_03_Building_RAG_Agent.ipynb\"\n",
    "try:\n",
    "    last_modified_time = os.path.getmtime(notebook_file)\n",
    "    last_modified_datetime = datetime.fromtimestamp(last_modified_time)\n",
    "    print(f\"Last Modified: {last_modified_datetime}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve last modified datetime: {e}\")\n",
    "\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import swarmauri\n",
    "    print(f\"Swarmauri Version: {swarmauri.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Swarmauri is not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0785d52-d6d4-4d89-89ed-5f4fd4f6e4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri(0.5)",
   "language": "python",
   "name": "swarmauri-0.5.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
