{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1848d67",
   "metadata": {},
   "source": [
    "# **Understanding Swarmauri Audio Processing Classes**\n",
    "---\n",
    "\n",
    "In this notebook, we’ll explore the various **Audio Processing Models** provided by Swarmauri.  \n",
    "\n",
    "We’ll also cover:  \n",
    "- How to view the allowed/supported models for each Audio Processing class.  \n",
    "- The methods available in these classes and their purposes.  \n",
    "\n",
    "This notebook serves as a comprehensive guide to understanding and working with Swarmauri’s Audio Processing classes, helping you make the most of its powerful features.  \n",
    "\n",
    "---\n",
    "\n",
    "## **What Are Audio Processing Models?**  \n",
    "\n",
    "Audio processing models are AI-driven systems designed to process and transform audio content. These models use cutting-edge deep learning techniques to perform tasks like converting audio to text or synthesizing text into natural-sounding speech.  \n",
    "\n",
    "### **Use Cases of Audio Processing Models**  \n",
    "Audio processing models have a wide range of applications across various industries. Below are some of the most notable use cases:  \n",
    "\n",
    "1. **Speech Recognition**  \n",
    "   - Transcribing podcasts, interviews, or meetings into text.  \n",
    "   - Assisting in closed captioning for videos and live broadcasts.  \n",
    "   - Enabling voice-controlled applications and devices.  \n",
    "\n",
    "2. **Text-to-Speech (TTS)**  \n",
    "   - Generating natural-sounding voiceovers for videos or presentations.  \n",
    "   - Assisting visually impaired users by reading text content aloud.  \n",
    "   - Creating interactive voice responses (IVRs) for customer service systems.  \n",
    "\n",
    "3. **Education and Training**  \n",
    "   - Providing audio-based learning tools for language acquisition.  \n",
    "   - Converting eBooks or written materials into audio formats for accessibility.  \n",
    "   - Enhancing virtual learning platforms with speech synthesis and transcription.  \n",
    "\n",
    "4. **Gaming and Entertainment**  \n",
    "   - Adding voiceovers to video games for characters or storytelling.  \n",
    "   - Creating realistic and expressive voices for animated characters.  \n",
    "   - Producing audiobooks or narrated content for streaming platforms.  \n",
    "\n",
    "5. **Accessibility**  \n",
    "   - Empowering individuals with disabilities by enabling voice-to-text or text-to-speech functionalities.  \n",
    "   - Assisting those with hearing impairments through real-time transcriptions.  \n",
    "   - Offering language translation and conversion for inclusivity.  \n",
    "\n",
    "\n",
    "By leveraging these use cases, businesses and individuals can unlock innovative ways to communicate, improve accessibility, and enhance user experiences.  \n",
    "\n",
    "With this understanding, let’s dive deeper into the specific **Audio Processing Models** provided by Swarmauri and the functionalities they offer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2cf3aa",
   "metadata": {},
   "source": [
    "## List of Audio Processing Classes in Swarmauri\n",
    "---\n",
    "\n",
    "Swarmauri provides the following Audio classes, named based on the providers of the models:\n",
    "\n",
    "1. **GroqAIAudio**: Converts Audio to Text. \n",
    "2. **HyperbolicAudioTTS**: Converts Text to Audio.\n",
    "3. **OpenAIAudio**: Converts Audio to Text.\n",
    "4. **OpenAIAudioTTS**: Converts Text to Audio.\n",
    "5. **PlayHTModel**: Converts Text to Audio.\n",
    "6. **WhisperLargeModel**: Converts Audio to Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27571a4a",
   "metadata": {},
   "source": [
    "## How to see the Allowed Models in any Audio class\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968a62b",
   "metadata": {},
   "source": [
    "1. ### Import the class \n",
    "\n",
    "- Here we will use GroqAIAudio, you can use any other class of your choice from the List of Audio Classes in Swarmauri "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd862dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.llms.concrete.GroqAIAudio import GroqAIAudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc7daf5",
   "metadata": {},
   "source": [
    "2. ### Instantiate the Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da879ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GroqAIAudio(api_key=\"put your api key here\") # Note: You don't need an API key to see the allowed_model, you can leave it as it is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ffeff",
   "metadata": {},
   "source": [
    "3. ### List all available models\n",
    "- To list the allowed models, we use the `allowed_models` class attribute, just like we did when working with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cabf6f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['distil-whisper-large-v3-en', 'whisper-large-v3']\n"
     ]
    }
   ],
   "source": [
    "available_models = model.allowed_models\n",
    "print(available_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8ac10e",
   "metadata": {},
   "source": [
    "As you can see, the allowed models for the `GroqAIAudio` class have been printed.  \n",
    "\n",
    "This approach is similar to how we worked with LLMs earlier. Swarmauri ensures consistency by allowing you to use the same methods and attributes across different classes in a unified manner. This design simplifies the process, making it more intuitive and efficient to build with Swarmauri."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b97aa",
   "metadata": {},
   "source": [
    "## Methods Available in Each Audio Processing Class  \n",
    "---\n",
    "\n",
    "Each **Audio Processing** class in **Swarmauri** provides a set of methods to interact with the models. These methods are designed to offer flexibility for both **synchronous** and **asynchronous** workflows, as well as batch processing. Additionally, **speech-to-text** classes support streaming functionalities, enabling real-time transcription.  \n",
    "\n",
    "### **Speech-to-Text (STT) Classes**  \n",
    "The following methods are available in classes like `GroqAIAudio`, `OpenAIAudio`, and `WhisperLargeModel`, which convert audio to text:  \n",
    "\n",
    "1. **`generate`**:  \n",
    "   - **Description**: Converts audio to text synchronously (blocking).  \n",
    "   - **Use Case**: When you need immediate transcription of an audio file.  \n",
    "\n",
    "2. **`agenerate`**:  \n",
    "   - **Description**: The asynchronous counterpart of `generate`. It allows you to transcribe audio without blocking the program.  \n",
    "   - **Use Case**: When running multiple transcription tasks concurrently using `asyncio`.  \n",
    "\n",
    "3. **`stream`**:  \n",
    "   - **Description**: Enables real-time transcription of audio input synchronously, providing partial text as audio is processed.  \n",
    "   - **Use Case**: When you require live transcription, such as for meetings, lectures, or broadcasts.  \n",
    "\n",
    "4. **`astream`**:  \n",
    "   - **Description**: Asynchronous version of the `stream` method for real-time transcription in an asynchronous environment.  \n",
    "   - **Use Case**: When you need live transcription while running other asynchronous tasks.  \n",
    "\n",
    "5. **`batch`**:  \n",
    "   - **Description**: Processes multiple audio-to-text requests in a synchronous manner.  \n",
    "   - **Use Case**: When you need to transcribe multiple audio files efficiently.  \n",
    "\n",
    "6. **`abatch`**:  \n",
    "   - **Description**: Asynchronous version of the `batch` method for transcribing multiple audio files concurrently.  \n",
    "   - **Use Case**: When you need to process a batch of transcription tasks in an asynchronous environment.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Text-to-Speech (TTS) Classes**  \n",
    "The following methods are available in classes like `HyperbolicAudioTTS`, `OpenAIAudioTTS`, and `PlayHTModel`, which convert text to audio:  \n",
    "\n",
    "1. **`generate`**:  \n",
    "   - **Description**: Converts text to audio synchronously (blocking).  \n",
    "   - **Use Case**: When you need immediate synthesis of a single text input into audio.  \n",
    "\n",
    "2. **`agenerate`**:  \n",
    "   - **Description**: The asynchronous counterpart of `generate`. It allows you to synthesize audio without blocking the program.  \n",
    "   - **Use Case**: When running multiple text-to-speech tasks concurrently using `asyncio`.  \n",
    "\n",
    "3. **`batch`**:  \n",
    "   - **Description**: Processes multiple text-to-audio requests in a synchronous manner.  \n",
    "   - **Use Case**: When you need to generate audio for a batch of text inputs efficiently.  \n",
    "\n",
    "4. **`abatch`**:  \n",
    "   - **Description**: Asynchronous version of the `batch` method for processing multiple text-to-speech requests concurrently.  \n",
    "   - **Use Case**: When you need to process a batch of TTS tasks in an asynchronous environment.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Note on Streaming**  \n",
    "- **Speech-to-Text Classes**: Support `stream` and `astream` methods, enabling real-time transcription for live audio.  \n",
    "- **Text-to-Speech Classes**: Do not support `stream` or `astream` methods because audio synthesis is delivered as a complete output rather than in incremental chunks.  \n",
    "\n",
    "This comprehensive method set ensures that Swarmauri’s Audio Processing classes are flexible and intuitive, catering to the unique demands of speech-to-text and text-to-speech workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad73f774-8c3b-49a3-9b9d-cd6f750c9817",
   "metadata": {},
   "source": [
    "# Notebook Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f860fc7-3b4b-4aee-8797-7a2d2e3254a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Victory Nnaji\n",
      "GitHub Username: 3rd-Son\n",
      "Notebook File: Notebook_01_Understanding_Swarmauri_Audio_Processing_Classes.ipynb\n",
      "Last Modified: 2024-12-30 12:17:33.997472\n",
      "Platform: Darwin 24.1.0\n",
      "Python Version: 3.11.11 (main, Dec 11 2024, 10:25:04) [Clang 14.0.6 ]\n",
      "Swarmauri Version: 0.5.2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.utils import print_notebook_metadata\n",
    "\n",
    "metadata = print_notebook_metadata.print_notebook_metadata(\"Victory Nnaji\", \"3rd-Son\")\n",
    "print(metadata) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri-notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
