{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to the Multimodal Translator Capstone**\n",
    "\n",
    "Welcome to the Multimodal Translator Capstone Project! This unique project integrates advanced language translation, text-to-speech, and image generation into one seamless workflow.\n",
    "\n",
    "#### **What You'll Learn**\n",
    "- **Multilingual Translation**: Harness the power of language models to translate text across various languages.\n",
    "- **Text-to-Speech**: Convert text into lifelike narration for enhanced accessibility and engagement.\n",
    "- **Contextual Imaging**: Generate relevant images based on translated content to provide visual context.\n",
    "- **Multimodal Integration**: Combining various AI and software components that process text, audio, and music into a seamless, end-to-end workflow.\"\n",
    "\n",
    "#### **Who Is This For?**\n",
    "- Developers with a foundation in Python looking to expand their AI skillset.\n",
    "- Enthusiasts aiming to integrate translation, audio, and image generation into real projects.\n",
    "- Anyone interested in harnessing state-of-the-art multimodal techniques.\n",
    "\n",
    "#### **Outcome**\n",
    "By the end of this capstone, you’ll have a fully functional Multimodal Translator that can translate text, generate audio pronunciation, and produce context-relevant images—showcasing your ability to build integrated AI solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Environment Setup**  \n",
    "- Imports required libraries (deep_translator, OpenAIAudioTTS, DeepInfraImgGenModel).  \n",
    "- Retrieves API keys from environment variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from deep_translator import GoogleTranslator\n",
    "from swarmauri.llms.concrete.OpenAIAudioTTS import OpenAIAudioTTS\n",
    "from swarmauri.llms.concrete.DeepInfraImgGenModel import DeepInfraImgGenModel\n",
    "from swarmauri.utils.base64_to_file_path import base64_to_file_path\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DEEPINFRA_API_KEY = os.getenv(\"DEEPINFRA_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Supported Languages Dictionary**  \n",
    "- Defines a dictionary mapping language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_LANGUAGES = {\n",
    "    'af': 'Afrikaans',\n",
    "    'ar': 'Arabic',\n",
    "    'hy': 'Armenian',\n",
    "    'az': 'Azerbaijani',\n",
    "    'be': 'Belarusian',\n",
    "    'bs': 'Bosnian',\n",
    "    'bg': 'Bulgarian',\n",
    "    'ca': 'Catalan',\n",
    "    'zh-cn': 'Chinese',  # Note: Using Simplified Chinese as default\n",
    "    'hr': 'Croatian',\n",
    "    'cs': 'Czech',\n",
    "    'da': 'Danish',\n",
    "    'nl': 'Dutch',\n",
    "    'en': 'English',\n",
    "    'et': 'Estonian',\n",
    "    'fi': 'Finnish',\n",
    "    'fr': 'French',\n",
    "    'gl': 'Galician',\n",
    "    'de': 'German',\n",
    "    'el': 'Greek',\n",
    "    'he': 'Hebrew',\n",
    "    'hi': 'Hindi',\n",
    "    'hu': 'Hungarian',\n",
    "    'is': 'Icelandic',\n",
    "    'id': 'Indonesian',\n",
    "    'it': 'Italian',\n",
    "    'ja': 'Japanese',\n",
    "    'kn': 'Kannada',\n",
    "    'kk': 'Kazakh',\n",
    "    'ko': 'Korean',\n",
    "    'lv': 'Latvian',\n",
    "    'lt': 'Lithuanian',\n",
    "    'mk': 'Macedonian',\n",
    "    'ms': 'Malay',\n",
    "    'mr': 'Marathi',\n",
    "    'mi': 'Maori',\n",
    "    'ne': 'Nepali',\n",
    "    'no': 'Norwegian',\n",
    "    'fa': 'Persian',\n",
    "    'pl': 'Polish',\n",
    "    'pt': 'Portuguese',\n",
    "    'ro': 'Romanian',\n",
    "    'ru': 'Russian',\n",
    "    'sr': 'Serbian',\n",
    "    'sk': 'Slovak',\n",
    "    'sl': 'Slovenian',\n",
    "    'es': 'Spanish',\n",
    "    'sw': 'Swahili',\n",
    "    'sv': 'Swedish',\n",
    "    'tl': 'Tagalog',\n",
    "    'ta': 'Tamil',\n",
    "    'th': 'Thai',\n",
    "    'tr': 'Turkish',\n",
    "    'uk': 'Ukrainian',\n",
    "    'ur': 'Urdu',\n",
    "    'vi': 'Vietnamese',\n",
    "    'cy': 'Welsh'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Text Translation**  \n",
    "- Defines `translate_text` function using GoogleTranslator.  \n",
    "- Validates input and returns either the translation or an error message.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, target_lang):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"Error: Invalid input text\"\n",
    "    \n",
    "    try:\n",
    "        translator = GoogleTranslator(source=\"auto\", target=target_lang)\n",
    "        translation = translator.translate(text)\n",
    "        return translation\n",
    "    except Exception as e:\n",
    "        return f\"Translation error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Pronunciation Generation**  \n",
    "- Defines `get_pronunciation` function using OpenAIAudioTTS.  \n",
    "- Generates an audio file for the provided text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronunciation(text, audio_path):\n",
    "    try:\n",
    "        tts_model = OpenAIAudioTTS(api_key=OPENAI_API_KEY)\n",
    "        audio_file_path = tts_model.predict(text=text, audio_path=audio_path)\n",
    "        return audio_file_path\n",
    "    except Exception as e:\n",
    "        return f\"Audio generation error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Contextual Image**  \n",
    "- Defines `get_contextual_image` to generate a base64-encoded image from text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contextual_image(query):\n",
    "    \n",
    "    try:\n",
    "        # For demonstration, using a placeholder image URL\n",
    "        image_gen_model = DeepInfraImgGenModel(api_key=DEEPINFRA_API_KEY, name=\"stabilityai/stable-diffusion-2-1\") \n",
    "        img = image_gen_model.generate_image_base64(query)\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        return f\"Image fetch error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: (Empty or Placeholder)**  \n",
    "- Can be used to add utility functions like language code retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_code(language_name: str) -> str:\n",
    "    \n",
    "    language_name = language_name.lower()\n",
    "    for code, name in SUPPORTED_LANGUAGES.items():\n",
    "        if name.lower() == language_name:\n",
    "            return code\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Multimodal Translation**  \n",
    "- Combines text translation, audio generation, and image creation into one function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that will handle the translation and audio/image generation\n",
    "def multimodal_translator(target_lang, text_input, source_audio_path=\"source_audio.mp3\", translated_audio_path=\"translated_audio.mp3\"):\n",
    "\n",
    "    target_lang_code = get_language_code(target_lang)\n",
    "\n",
    "    translated_text = translate_text(text_input, target_lang_code)\n",
    "\n",
    "    source_audio = get_pronunciation(text_input, source_audio_path)\n",
    "    translated_audio = get_pronunciation(translated_text, translated_audio_path)\n",
    "\n",
    "    image_url = get_contextual_image(translated_text)\n",
    "\n",
    "    image = base64_to_file_path(image_url, \"image.png\")\n",
    "    \n",
    "    return source_audio, translated_audio, image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Step 8: Gradio UI**  \n",
    "- Creates a Gradio interface for user input.  \n",
    "- Displays outputs for translation, audio playback, and image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            # Target Language Dropdown\n",
    "            target_lang = gr.Dropdown(\n",
    "                label=\"Target Language\",\n",
    "                choices=list(SUPPORTED_LANGUAGES.values()),\n",
    "                value=\"Spanish\"\n",
    "            )\n",
    "            # Text Input\n",
    "            text_input = gr.Textbox(label=\"Text Input\")\n",
    "            # Submit Button\n",
    "            submit_btn = gr.Button(\"Translate and Generate\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            # Source Audio Output\n",
    "            source_audio = gr.Audio(label=\"Source Audio\", type=\"filepath\")\n",
    "            # Translated Audio Output\n",
    "            translated_audio = gr.Audio(label=\"Translated Audio\", type=\"filepath\")\n",
    "            # Image Output\n",
    "            image_output = gr.Image(label=\"Generated Image\", type=\"filepath\")\n",
    "    \n",
    "    # Connect the button click to the function\n",
    "    submit_btn.click(\n",
    "        fn=multimodal_translator,\n",
    "        inputs=[target_lang, text_input],\n",
    "        outputs=[source_audio, translated_audio, image_output]\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(show_error=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri_notebook_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
