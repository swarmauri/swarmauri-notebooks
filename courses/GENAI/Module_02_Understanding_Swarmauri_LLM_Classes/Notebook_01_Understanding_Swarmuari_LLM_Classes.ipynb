{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Understanding Swarmauri LLM Classes**\n",
    "---\n",
    "\n",
    "In this notebook, we’ll see the various **LLMs** (Language Models) provided by Swarmauri  \n",
    "\n",
    "We’ll also cover:  \n",
    "- How to view the allowed/supported models for each LLM class.  \n",
    "- The methods available in these classes and their purposes.  \n",
    "\n",
    "This notebook serves as a comprehensive guide to understanding and working with Swarmauri’s llm classes, helping you make the most of its powerful features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of LLM Classes in Swarmauri\n",
    "---\n",
    "\n",
    "Swarmauri provides the following LLM classes, named based on the providers of the models:\n",
    "\n",
    "1. **AI21Model**  \n",
    "2. **AnthropicModel**  \n",
    "3. **CohereModel**  \n",
    "4. **DeepInfraModel**  \n",
    "5. **DeepSeekModel**  \n",
    "6. **GeminiProModel**  \n",
    "7. **GroqModel**  \n",
    "8. **HyperbolicModel**  \n",
    "9. **MistralModel**  \n",
    "10. **OpenAIModel**  \n",
    "11. **PerplexityModel**  \n",
    "\n",
    "\n",
    "### Provider Naming Convention  \n",
    "Swarmauri follows a *provider naming convention*. This means that the file and class names reflect the **provider** of the LLMs (e.g., `OpenAIModel` for OpenAI) rather than the specific model names (e.g., `GPT-4`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to see the Allowed Models in each LLM class\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ### Import the class \n",
    "\n",
    "- Here we will use MistralModel, you can use any other class of your choice from the List of LLM Classes in Swarmauri "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.llms.concrete.MistralModel import MistralModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ### Instantiate the Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MistralModel(api_key=\" your api key here\") # Note: You don't need an API key to see the allowed_model, you can leave it as it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ### List all available models\n",
    "- To list the allowed models, we use the `allowed_models` class attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['open-mistral-7b', 'open-mixtral-8x7b', 'open-mixtral-8x22b', 'mistral-small-latest', 'mistral-medium-latest', 'mistral-large-latest', 'open-mistral-nemo', 'codestral-latest', 'open-codestral-mamba']\n"
     ]
    }
   ],
   "source": [
    "available_models = model.allowed_models\n",
    "print(available_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this printed the allowed models. You can apply this approach to other classes by importing the respective model class (e.g., `OpenAIModel`, `AnthropicModel`, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods Available in Each LLM Class  \n",
    "---\n",
    "\n",
    "Each LLM class in **Swarmauri** provides the following methods for interacting with the models. These methods are designed to offer flexibility for both **synchronous** and **asynchronous** workflows, as well as batch processing.  \n",
    "\n",
    "1. **`predict`**:  \n",
    "   - **Description**: This method allows you to interact with the LLM in a synchronous (blocking) way.  \n",
    "   - **Use Case**: When you need immediate results for a single input.  \n",
    "\n",
    "\n",
    "2. **`apredict`**:  \n",
    "   - **Description**: The asynchronous counterpart of `predict`. It allows you to interact with the LLM without blocking the program.  \n",
    "   - **Use Case**: When running multiple tasks concurrently using `asyncio`.  \n",
    "\n",
    "\n",
    "3. **`stream`**:  \n",
    "   - **Description**: Enables streaming responses from the LLM synchronously. This is useful for real-time text generation.  \n",
    "   - **Use Case**: When you want to process or display results as they are generated.  \n",
    "\n",
    "\n",
    "4. **`astream`**:  \n",
    "   - **Description**: Asynchronous streaming of responses. It allows you to stream responses without blocking the program.  \n",
    "   - **Use Case**: For real-time streaming in an asynchronous environment.  \n",
    "\n",
    "\n",
    "5. **`batch`**:  \n",
    "   - **Description**: Allows you to process multiple inputs at once in a synchronous manner.  \n",
    "   - **Use Case**: When you need to send a batch of requests for efficiency.  \n",
    "\n",
    "\n",
    "6. **`abatch`**:  \n",
    "   - **Description**: Asynchronous version of the `batch` method for processing multiple inputs concurrently.  \n",
    "   - **Use Case**: When you need to process a batch of inputs in an asynchronous environment.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Table of Methods  \n",
    "\n",
    "| **Method**   | **Type**         | **Description**                                | **Use Case**                          |  \n",
    "|--------------|------------------|-----------------------------------------------|---------------------------------------|  \n",
    "| `predict`    | Synchronous      | Standard single-input LLM interaction.        | Immediate, single input results.      |  \n",
    "| `apredict`   | Asynchronous     | Non-blocking single-input interaction.        | Concurrent tasks with async.          |  \n",
    "| `stream`     | Synchronous      | Streaming responses in real time.             | Real-time output for synchronous use. |  \n",
    "| `astream`    | Asynchronous     | Streaming responses asynchronously.           | Real-time async output.               |  \n",
    "| `batch`      | Synchronous      | Processes multiple inputs synchronously.      | Batch processing for efficiency.      |  \n",
    "| `abatch`     | Asynchronous     | Processes multiple inputs asynchronously.     | Async batch processing.               |  \n",
    "\n",
    "These methods provide the necessary tools for diverse workflows, whether you need synchronous, asynchronous, or batch interactions with the LLM.  \n",
    "\n",
    "Sit tight for the next part, where we'll dive into the implementation details of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTEBOOK METADATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Victory Nnaji\n",
      "GitHub Username: 3rd-Son\n",
      "Notebook File: Notebook_01_Understanding_Swarmuari_LLM_Classes.ipynb\n",
      "Last Modified: 2024-12-20 15:41:11.743716\n",
      "Platform: Darwin 24.1.0\n",
      "Python Version: 3.11.11 (main, Dec 11 2024, 10:25:04) [Clang 14.0.6 ]\n",
      "Swarmauri Version: 0.5.2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.utils import print_notebook_metadata\n",
    "\n",
    "metadata = print_notebook_metadata.print_notebook_metadata(\"Victory Nnaji\", \"3rd-Son\")\n",
    "print(metadata) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri-notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
