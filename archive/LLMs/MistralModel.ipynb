{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Using the Swarmauri Package Components**\n",
    "\n",
    "This guide demonstrates how to use various components of the Swarmauri package, including the `MistralModel`, `Conversation`, and message classes such as `HumanMessage`, `SystemMessage`. These examples will guide you through the setup and utilization of these components for common tasks.\n",
    "\n",
    "## **Setup**\n",
    "\n",
    "Before using the components, ensure you have the necessary API keys set up as environment variables. In this case, you'll need the `MISTRAL_API_KEY` for initializing the `MistralModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from swarmauri.standard.llms.concrete.MistralModel import MistralModel as LLM\n",
    "from swarmauri.standard.conversations.concrete.Conversation import Conversation\n",
    "\n",
    "from swarmauri.standard.messages.concrete.HumanMessage import HumanMessage\n",
    "from swarmauri.standard.messages.concrete.SystemMessage import SystemMessage\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using the MistralModel**\n",
    "\n",
    "Let's start by instantiating the AnthropicToolModel and exploring some of its properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv('MISTRAL_API_KEY')\n",
    "llm = LLM(api_key = API_KEY)\n",
    "\n",
    "\n",
    "# Check the resource type\n",
    "llm.resource\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resource property indicates the type of resource the model represents. In this case, it is LLM (Large Language Model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MistralModel'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type property tells us the specific type of model we are working with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llm.id == LLM.model_validate_json(llm.model_dump_json()).id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialization and deserialization allow us to convert the model to a JSON representation and back, ensuring data integrity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'open-mixtral-8x7b'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llm.name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name property provides the default name of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using Conversation, HumanMessage**\n",
    "\n",
    "Next, let's initialise  a conversation, adds a human message, and uses the language model to generate a response based on the conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new conversation instance to hold the sequence of messages.\n",
    "conversation = Conversation()\n",
    "\n",
    "# Define the user's input as \"Hello\" and create a HumanMessage with this content.\n",
    "input_data = \"Hello\"\n",
    "human_message = HumanMessage(content=input_data)\n",
    "\n",
    "\n",
    "# Add the human message to the conversation. This simulates the user saying \"Hello\".\n",
    "conversation.add_message(human_message)\n",
    "\n",
    "# Use the LLM (Language Model) to generate a response based on the conversation so far.\n",
    "llm.predict(conversation=conversation)\n",
    "\n",
    "# Retrieve the content of the last message in the conversation, which should be the model's response.\n",
    "prediction = conversation.get_last().content\n",
    "\n",
    "\n",
    "type(prediction) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using the Conversation, HumanMessage and SystemMessage Components**\n",
    "\n",
    "Let's use the Conversation and SystemMessage components to guide model responses.\n",
    "\n",
    "Start by creating a Conversation object and adding a human message, like \"Hi\". Generate a response from the model based on this initial input.\n",
    "\n",
    "Next, introduce a SystemMessage to instruct the model to respond with the phrase \"Jeff\". Add another human message to the conversation, such as \"Hello Again.\", and generate a new model response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation = Conversation()\n",
    "\n",
    "# Say hi\n",
    "input_data = \"Hi\"\n",
    "human_message = HumanMessage(content=input_data)\n",
    "conversation.add_message(human_message)\n",
    "\n",
    "    # Get Prediction\n",
    "prediction = llm.predict(conversation=conversation)\n",
    "\n",
    "    # Give System Context\n",
    "system_context = 'You only respond with the following phrase, \"Jeff\"'\n",
    "human_message = SystemMessage(content=system_context)\n",
    "conversation.add_message(human_message)\n",
    "\n",
    "    # Prompt\n",
    "input_data = \"Hello Again.\"\n",
    "human_message = HumanMessage(content=input_data)\n",
    "conversation.add_message(human_message)\n",
    "\n",
    "\n",
    "llm.predict(conversation=conversation)\n",
    "prediction = conversation.get_last().content\n",
    "'Jeff' in prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using SystemMessage to Control Model Responses**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conversation = Conversation()\n",
    "\n",
    "system_context = 'You only respond with the following phrase, \"Jeff\"'\n",
    "human_message = SystemMessage(content=system_context)\n",
    "conversation.add_message(human_message)\n",
    "\n",
    "input_data = \"Hi\"\n",
    "human_message = HumanMessage(content=input_data)\n",
    "conversation.add_message(human_message)\n",
    "\n",
    "llm.predict(conversation=conversation)\n",
    "prediction = conversation.get_last().content\n",
    "type(prediction) == str\n",
    "'Jeff' in prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's guide the model's response using a SystemMessage within a Conversation. We start by initializing a Conversation object to manage the dialogue, then add a SystemMessage that instructs the model to only respond with the phrase \"Jeff.\" This sets a specific context that the model should follow. Next, we simulate user input by adding a human message with the content \"Hi.\" The language model then generates a response based on the conversation's current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handling of Multiple System Contexts in Model Responses**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation = Conversation()\n",
    "\n",
    "system_context = 'You only respond with the following phrase, \"Jeff\"'\n",
    "human_message = SystemMessage(content=system_context)\n",
    "conversation.add_message(human_message)\n",
    "\n",
    "input_data = \"Hi\"\n",
    "human_message = HumanMessage(content=input_data)\n",
    "conversation.add_message(human_message)\n",
    "\n",
    "llm.predict(conversation=conversation)\n",
    "\n",
    "system_context_2 = 'You only respond with the following phrase, \"Ben\"'\n",
    "human_message = SystemMessage(content=system_context_2)\n",
    "conversation.add_message(human_message)\n",
    "\n",
    "input_data_2 = \"Hey\"\n",
    "human_message = HumanMessage(content=input_data_2)\n",
    "conversation.add_message(human_message)\n",
    "\n",
    "llm.predict(conversation=conversation)\n",
    "prediction = conversation.get_last().content\n",
    "type(prediction) == str\n",
    "'Ben' in prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we introduce multiple system contexts within a conversation to test if the model correctly updates its responses based on the latest context. Initially, the model is instructed to respond with \"Jeff\" when prompted, but later, the context is switched to \"Ben.\" The test checks if the model's final response aligns with the most recent context update, ensuring it correctly handles dynamic context changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "In this notebook, we have explored how to effectively use components from the Swarmauri package, including the MistralModel, Conversation, AgentMessage, HumanMessage, and SystemMessage. These components allow us to create a flexible and context-aware conversational model that can handle complex interactions. By guiding the model's responses through dynamic system contexts and validating its output, we have demonstrated how to control and adapt the model's behavior within a conversation, ensuring it responds accurately and according to specified instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
